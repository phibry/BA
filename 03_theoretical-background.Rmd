---
geometry: margin = 1in
output:
  html_document:
    df_print: paged
---

## 2. Theory

Bitcoin explanation in section [2.3.](#bitcoin)

Ken is testing Github. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Commodo elit at imperdiet dui accumsan sit. Tempus iaculis urna id volutpat lacus laoreet non curabitur gravida. Turpis egestas integer eget aliquet nibh praesent tristique magna. Nec ultrices dui sapien eget mi proin sed. Turpis nunc eget lorem dolor sed viverra ipsum nunc. Lorem donec massa sapien faucibus et. In hac habitasse platea dictumst vestibulum rhoncus est pellentesque. Ac tortor vitae purus faucibus ornare suspendisse sed nisi lacus. Mauris cursus mattis molestie a iaculis at erat pellentesque adipiscing.

### 2.1. Neural Network

Set of algorithms to recognize pattern. Mapping input to output. Three different applications

-   Classification (Supervised Learning): Depending on labeled data (Human groups them, or result of another method). NN finds correlation between labels and data. -\> **identify objects in imgs, gesture recognition, detect voices, classify text as spam**
-   Clustering (Unsupervised Learning): Detection of similarities, grouping them together. Unlabeled data. The more data, the more accurate is the output. -\> **fraud detection, comparing docs/imgs, anomaly detection**
-   Predictive Analysis: Exposed to enough data, NN is able to establish correlation between present events and future events. Regression between the past and the future. -\> **Time Series, predict the number which most likely to occur next**

Further explanation: Nodes, Layer, Input, Ouput, Weights, Activation funtion, Training, Feedforward NN, Mulitple linear Regression, Gradient Descent, Logistic Regression

### 2.2. LSTM / RNN

Long-short-term-memory / Recurrent Neural Networks

Explain.

### 2.3. Bitcoin {#bitcoin}

#### 2.3.1. SHA256 Hash

 

-   Block
-   Blockchain
-   Distributed Blockchain
-   Token
-   Coinbase Transaction
-   Public/Private Key -\> Signing
-   Signature (sign, verify)
-   Transaction

\newpage

### 2.7. Reference

#### 2.7.1. Figure Reference

 

As you can see in figure \ref{fig:fig1} -\> GOOGLE

```{r fig1, echo=FALSE, fig.cap="Visualization of the adjusted prices of the Alphabet Inc Class A Stock."}
GOOGL <- getSymbols("GOOGL", auto.assign=F)
g.adj <- GOOGL[,6]
par(mfrow=c(1,1))
plot(g.adj, main="Adjusted Prices ~ Google")
```

\newpage

#### 2.7.2. Equation Reference

 

GARCH-formula can be seen in \ref{eq:garch}

```{=tex}
\begin{align} \label{eq:garch}
  \epsilon_{t} &= \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1}) \nonumber \\
  \epsilon_{t} &= \sigma_{t}u_{t} \\
  \sigma_{t}^{2} &=c \sigma^{2}+\sum_{j=1}^{n}\alpha_{j}\sigma_{t-j}^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \nonumber
\end{align}
```
\newpage

#### 2.7.3. Table Reference

 

In table \ref{tab:coeftable} you can see the flexerino of the coefficients.

```{r coeftable, echo=FALSE, message=FALSE}
g.adj.lr <- na.exclude(diff(log(g.adj)))
g.subset.lr <- head(g.adj.lr, length(g.adj.lr)-100)
y.garch_11 <- garchFit(~garch(1,1), data=g.subset.lr, delta=2, include.delta=F, 
                       include.mean=F, trace=F)
r1 <- y.garch_11@fit$matcoef[,1]
r2 <- y.garch_11@fit$matcoef[,2]
r3 <- y.garch_11@fit$matcoef[,4]
paras <- data.frame(r1, r2, r3)
rownames(paras) <- c("$\\omega$", "$\\alpha_{1}$", "$\\beta_{1}$")
colnames(paras) <- c("Estimate", "Std. Error", "p-Value")
kable(paras, "latex", escape=F, booktabs = T, linesep="", caption="Coefficients GARCH(1,1).", digits=20)
```

\newpage

#### 2.7.4. Section Reference {#sec-ref}

Here we can see a wild section, which will reference to itself: [2.7.4.](#sec-ref)

Or reference to the Bitcoin-section [2.3.](#bitcoin)

\newpage

#### 2.7.5. Literature Reference

Add bibliography reference in the `.bib`-file in the add folder.

Here I make a reference to the original bitcoinpaper [@bitcoin] `[@bitcoin]` or to the specific page on the NN financial trading paper [@nnfin, pp. 6-8] -\> `[@nnfin, pp. 6-8]`
