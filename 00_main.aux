\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {subsection}{Abstract}{}{section*.2}\protected@file@percent }
\newlabel{abstract}{{}{}{Abstract}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Zusammenfassung}{}{section*.3}\protected@file@percent }
\newlabel{zusammenfassung}{{}{}{Zusammenfassung}{section*.3}{}}
\@writefile{toc}{\contentsline {subsection}{1. Introduction}{1}{section*.4}\protected@file@percent }
\newlabel{introduction}{{}{1}{1. Introduction}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{2. Theory}{2}{section*.5}\protected@file@percent }
\newlabel{theory}{{}{2}{2. Theory}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.1. Neural network}{2}{section*.6}\protected@file@percent }
\newlabel{neural_network}{{}{2}{2.1. Neural network}{section*.6}{}}
\newlabel{perceptron}{{}{2}{2.1.1. Perceptron}{section*.7}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.1. Perceptron}{2}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a perceptron. Input data are added up with weights to a sum. An error term (bias) is then added and sent through the activation function to produce the output.\relax }}{3}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron_schema}{{1}{3}{Schematic diagram of a perceptron. Input data are added up with weights to a sum. An error term (bias) is then added and sent through the activation function to produce the output.\relax }{figure.caption.8}{}}
\newlabel{eq:perceptron}{{1}{3}{2.1.1. Perceptron}{equation.0.1}{}}
\newlabel{backprogation_algorithm}{{}{4}{2.1.2. Backpropagation Algorithm}{section*.9}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.2. Backpropagation Algorithm}{4}{section*.9}\protected@file@percent }
\newlabel{eq:sigmoid_logistic}{{2}{4}{2.1.2. Backpropagation Algorithm}{equation.0.2}{}}
\newlabel{eq:sigmoid_tanh}{{3}{4}{2.1.2. Backpropagation Algorithm}{equation.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two common non-linear sigmoid activation functions: In black, the logistic function, which returns values between 0 and 1. In red the hyperbolic tangent function, which returns values between -1 and 1.\relax }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sigmoid}{{2}{4}{Two common non-linear sigmoid activation functions: In black, the logistic function, which returns values between 0 and 1. In red the hyperbolic tangent function, which returns values between -1 and 1.\relax }{figure.caption.10}{}}
\newlabel{eq:mse}{{4}{5}{2.1.2. Backpropagation Algorithm}{equation.0.4}{}}
\newlabel{eq:gradient_descent}{{5}{5}{2.1.2. Backpropagation Algorithm}{equation.0.5}{}}
\newlabel{eq:weight_adj}{{6}{5}{2.1.2. Backpropagation Algorithm}{equation.0.6}{}}
\newlabel{MLP}{{}{6}{2.1.3. Multilayer Perceptron}{section*.11}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.3. Multilayer Perceptron}{6}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of a multilayer perceptron. On the left, you see the input layer, on the right the output layer. All layers in between are described as hidden layers. If there is more than one hidden layer, it is referred to as a deep learning neural network.\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mlp_schema}{{3}{6}{Schematic diagram of a multilayer perceptron. On the left, you see the input layer, on the right the output layer. All layers in between are described as hidden layers. If there is more than one hidden layer, it is referred to as a deep learning neural network.\relax }{figure.caption.12}{}}
\newlabel{challenges}{{}{7}{2.1.4. Challenges}{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.4. Challenges}{7}{section*.13}\protected@file@percent }
\newlabel{overfitting}{{}{7}{2.1.4.1. Overfitting}{section*.14}{}}
\@writefile{toc}{\contentsline {subparagraph}{2.1.4.1. Overfitting}{7}{section*.14}\protected@file@percent }
\newlabel{vanishing_gradient}{{}{7}{2.1.4.2. Vanishing Gradient Problem}{section*.15}{}}
\@writefile{toc}{\contentsline {subparagraph}{2.1.4.2. Vanishing Gradient Problem}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2.2. Explainable Artificial Intelligence}{8}{section*.16}\protected@file@percent }
\newlabel{xai}{{}{8}{2.2. Explainable Artificial Intelligence}{section*.16}{}}
\newlabel{classic}{{}{8}{2.2.1. Classic Approach}{section*.17}{}}
\@writefile{toc}{\contentsline {paragraph}{2.2.1. Classic Approach}{8}{section*.17}\protected@file@percent }
\newlabel{xai_finance}{{}{9}{2.2.2. Explainability for Financial Time Series}{section*.18}{}}
\@writefile{toc}{\contentsline {paragraph}{2.2.2. Explainability for Financial Time Series}{9}{section*.18}\protected@file@percent }
\newlabel{eq:lm1}{{7}{9}{2.2.2. Explainability for Financial Time Series}{equation.0.7}{}}
\newlabel{eq:lm2}{{8}{9}{2.2.2. Explainability for Financial Time Series}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Autocorrelation function of BTC/USD. In red and orange you can see significant positive ACF's at lags 6 and 10. These signal some importance for model considerations. Between 6 and 10, the ACF's are negative and thus would have a negative effect on a model. For further considerations, all lags from 6 to 10 are included.\relax }}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig:xai_acf}{{4}{9}{Autocorrelation function of BTC/USD. In red and orange you can see significant positive ACF's at lags 6 and 10. These signal some importance for model considerations. Between 6 and 10, the ACF's are negative and thus would have a negative effect on a model. For further considerations, all lags from 6 to 10 are included.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Coefficients 6 (red) and 10 (orange) have the greatest positive impact on the model. In between, the coefficients are negative.\relax }}{10}{table.caption.20}\protected@file@percent }
\newlabel{tab:lm_tab}{{1}{10}{Coefficients 6 (red) and 10 (orange) have the greatest positive impact on the model. In between, the coefficients are negative.\relax }{table.caption.20}{}}
\newlabel{eq:xai_partial}{{9}{10}{2.2.2. Explainability for Financial Time Series}{equation.0.9}{}}
\newlabel{eq:xai_fit}{{10}{10}{2.2.2. Explainability for Financial Time Series}{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Upper panel: LPD's of BTC's log returns. Only lags 6 to 10 are highlighted in color. Lag 6 (red) and 10 (orange) have the largest positive impact. 7, 8 and 9 have a negative impact. Lower panel: Logarithmic returns of BTC. The volatile phases can be seen very well again at the top of the LPD's.\relax }}{11}{figure.caption.21}\protected@file@percent }
\newlabel{fig:xai_plot1}{{5}{11}{Upper panel: LPD's of BTC's log returns. Only lags 6 to 10 are highlighted in color. Lag 6 (red) and 10 (orange) have the largest positive impact. 7, 8 and 9 have a negative impact. Lower panel: Logarithmic returns of BTC. The volatile phases can be seen very well again at the top of the LPD's.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison between linear regression (LR) and linear parameter data (LPD) coefficients. Again, lags 6 and 10 have the largest positive effect in the respective model. Also 7, 8 and 9 all have a negative sign.\relax }}{12}{table.caption.22}\protected@file@percent }
\newlabel{tab:lpd_tab}{{2}{12}{Comparison between linear regression (LR) and linear parameter data (LPD) coefficients. Again, lags 6 and 10 have the largest positive effect in the respective model. Also 7, 8 and 9 all have a negative sign.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.3. Model Comparison}{13}{section*.23}\protected@file@percent }
\newlabel{model_comparison}{{}{13}{2.3. Model Comparison}{section*.23}{}}
\newlabel{sharpe_ratio}{{}{13}{2.3.1. Sharpe Ratio}{section*.24}{}}
\@writefile{toc}{\contentsline {paragraph}{2.3.1. Sharpe Ratio}{13}{section*.24}\protected@file@percent }
\newlabel{eq:Sharpe}{{11}{13}{2.3.1. Sharpe Ratio}{equation.0.11}{}}
\newlabel{MSE}{{}{13}{2.3.2. Mean Squared Error (MSE)}{section*.25}{}}
\@writefile{toc}{\contentsline {paragraph}{2.3.2. Mean Squared Error (MSE)}{13}{section*.25}\protected@file@percent }
\newlabel{eq:MSE}{{12}{13}{2.3.2. Mean Squared Error (MSE)}{equation.0.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.4. Bitcoin}{14}{section*.26}\protected@file@percent }
\newlabel{bitcoin}{{}{14}{2.4. Bitcoin}{section*.26}{}}
\newlabel{historical_analysis}{{}{14}{2.4.1. Historical Analysis}{section*.27}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.1. Historical Analysis}{14}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Price development of BTC in USD from fall 2014 to spring 2021. Events worth mentioning are marked with letters A to E and described in the text above.\relax }}{15}{figure.caption.28}\protected@file@percent }
\newlabel{fig:price_btc}{{6}{15}{Price development of BTC in USD from fall 2014 to spring 2021. Events worth mentioning are marked with letters A to E and described in the text above.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Logarithmic transformation of the price development of the upper chart. In contrast to the upper chart, here you can see not only the extreme price spikes, but also the rather small price movements.\relax }}{15}{figure.caption.29}\protected@file@percent }
\newlabel{fig:logprice_btc}{{7}{15}{Logarithmic transformation of the price development of the upper chart. In contrast to the upper chart, here you can see not only the extreme price spikes, but also the rather small price movements.\relax }{figure.caption.29}{}}
\newlabel{bitcoin_tec}{{}{16}{2.4.2. Cryptocurrencies and Bitcoin Technology}{section*.30}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.2. Cryptocurrencies and Bitcoin Technology}{16}{section*.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Schematic diagram of a blockchain. In black are the previous blocks. This is followed by a fork in red and green. The red block is wrong and is not accepted, because the green path is longer and only this one is trusted.\relax }}{16}{figure.caption.31}\protected@file@percent }
\newlabel{fig:blockchain}{{8}{16}{Schematic diagram of a blockchain. In black are the previous blocks. This is followed by a fork in red and green. The red block is wrong and is not accepted, because the green path is longer and only this one is trusted.\relax }{figure.caption.31}{}}
\newlabel{bitcoin_valuation}{{}{17}{2.4.3. Valuation and Digital Gold}{section*.32}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.3. Valuation and Digital Gold}{17}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{3. Methodology}{18}{section*.33}\protected@file@percent }
\newlabel{methodology}{{}{18}{3. Methodology}{section*.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces This flowchart illustrates an overview of the individual intermediate steps that are covered in the Methodology section. \relax }}{18}{figure.caption.34}\protected@file@percent }
\newlabel{fig:flowchart_overview}{{9}{18}{This flowchart illustrates an overview of the individual intermediate steps that are covered in the Methodology section. \relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.1. Data Exploration}{19}{section*.35}\protected@file@percent }
\newlabel{data_exploration}{{}{19}{3.1. Data Exploration}{section*.35}{}}
\newlabel{eq:logreturn}{{14}{19}{3.1. Data Exploration}{equation.0.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Logarithmic returns of BTC/USD. Start of the time series is autumn 2014 and goes here until spring 2021. One can recognize the typical patterns of log returns, namely the vola clusters. This means phases in which the returns fluctuate strongly.\relax }}{19}{figure.caption.36}\protected@file@percent }
\newlabel{fig:log_ret}{{10}{19}{Logarithmic returns of BTC/USD. Start of the time series is autumn 2014 and goes here until spring 2021. One can recognize the typical patterns of log returns, namely the vola clusters. This means phases in which the returns fluctuate strongly.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Autocorrelation function of BTC log returns during the entire time period from fall 2014 to spring 2021. Lag 6 and 10 are significant (values exceed the blue dotted line) and thus have an impact on possible models.\relax }}{20}{figure.caption.37}\protected@file@percent }
\newlabel{fig:acf_log_ret}{{11}{20}{Autocorrelation function of BTC log returns during the entire time period from fall 2014 to spring 2021. Lag 6 and 10 are significant (values exceed the blue dotted line) and thus have an impact on possible models.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Histogram of BTC log return. Possible distribution of the data. None of the possible distributions would fit the data as well as possible. The blue t-Student distribution would fit the data better in the tails, while the yellow normal distribution would disperse a little better around the expected value.\relax }}{21}{figure.caption.38}\protected@file@percent }
\newlabel{fig:histogramm_logreturns}{{12}{21}{Histogram of BTC log return. Possible distribution of the data. None of the possible distributions would fit the data as well as possible. The blue t-Student distribution would fit the data better in the tails, while the yellow normal distribution would disperse a little better around the expected value.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.2. Network Architecture}{21}{section*.39}\protected@file@percent }
\newlabel{networkarchitecture}{{}{21}{3.2. Network Architecture}{section*.39}{}}
\newlabel{eq:comb}{{15}{21}{3.2. Network Architecture}{equation.0.15}{}}
\newlabel{test_train}{{}{23}{3.2.1. Defining Train and Test Samples}{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.1. Defining Train and Test Samples}{23}{section*.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Nine different in- and out-of-sample splits. The 6-month training phase (in-sample) is indicated in blue and the 1-month test phase (out-of-sample) in green. The splits capture different phases of the time series. Both very turbulent phases (split 1) and rather calm phases (split 5) can be recorded.\relax }}{23}{figure.caption.41}\protected@file@percent }
\newlabel{fig:test_train}{{13}{23}{Nine different in- and out-of-sample splits. The 6-month training phase (in-sample) is indicated in blue and the 1-month test phase (out-of-sample) in green. The splits capture different phases of the time series. Both very turbulent phases (split 1) and rather calm phases (split 5) can be recorded.\relax }{figure.caption.41}{}}
\newlabel{input-layer}{{}{24}{3.2.2. Defining Input-Layer}{section*.42}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.2. Defining Input-Layer}{24}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Autocorrelation function of our chosen time period. One can clearly see an increased influence of lags 4 and 7, not as in the ACFs of the entire data set with significant values at 6 and 10. Only late at lag 18 does the ACF show a significant value again.\relax }}{24}{figure.caption.43}\protected@file@percent }
\newlabel{fig:test_train_acf}{{14}{24}{Autocorrelation function of our chosen time period. One can clearly see an increased influence of lags 4 and 7, not as in the ACFs of the entire data set with significant values at 6 and 10. Only late at lag 18 does the ACF show a significant value again.\relax }{figure.caption.43}{}}
\newlabel{train-nn}{{}{25}{3.x.x. Neural Network Training}{section*.44}{}}
\@writefile{toc}{\contentsline {paragraph}{3.x.x. Neural Network Training}{25}{section*.44}\protected@file@percent }
\newlabel{evaluate_nn}{{}{26}{3.2.3. Evaluating Network Architecture}{section*.45}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.3. Evaluating Network Architecture}{26}{section*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces In- and out-of-sample MSE of the 5th split. Only the networks with only one layer are shown here. Simplest network has 1 neuron (leftmost) and the most complex one has 10 neurons (rightmost), so here you have the MSEs of 10 different networks. In-sample the MSEs tend to decrease, while out-of-sample they increase.\relax }}{26}{figure.caption.46}\protected@file@percent }
\newlabel{fig:mse_plot1}{{15}{26}{In- and out-of-sample MSE of the 5th split. Only the networks with only one layer are shown here. Simplest network has 1 neuron (leftmost) and the most complex one has 10 neurons (rightmost), so here you have the MSEs of 10 different networks. In-sample the MSEs tend to decrease, while out-of-sample they increase.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces In- and out-of-sample MSE of the 5th split. Shown here are only the networks with two layers. The simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), so you have the MSEs of 100 different networks. As in the previous graph, in-sample MSEs tend to decrease while out-of-sample MSEs tend to increase.\relax }}{27}{figure.caption.47}\protected@file@percent }
\newlabel{fig:mse_plot2}{{16}{27}{In- and out-of-sample MSE of the 5th split. Shown here are only the networks with two layers. The simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), so you have the MSEs of 100 different networks. As in the previous graph, in-sample MSEs tend to decrease while out-of-sample MSEs tend to increase.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces In- and out-of-sample MSE of the 5th split. Shown here are only the networks with 3 layers. Simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), so here you have the MSEs of 1000 different networks. Same pattern as for 1 layer and 2 layers. In-sample the MSE decreases, out-of-sample it increases.\relax }}{28}{figure.caption.48}\protected@file@percent }
\newlabel{fig:mse_plot3}{{17}{28}{In- and out-of-sample MSE of the 5th split. Shown here are only the networks with 3 layers. Simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), so here you have the MSEs of 1000 different networks. Same pattern as for 1 layer and 2 layers. In-sample the MSE decreases, out-of-sample it increases.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces In- and out-of-sample MSE of the 5th split. The 3 previous graphs are summarized here. One has here an overview of all 3 layers and thus of all 1110 neuron-layer combinations. The more complex a network, the smaller the in-sample MSE becomes, but the larger the out-of-sample MSE becomes.\relax }}{29}{figure.caption.49}\protected@file@percent }
\newlabel{fig:mse_plot5}{{18}{29}{In- and out-of-sample MSE of the 5th split. The 3 previous graphs are summarized here. One has here an overview of all 3 layers and thus of all 1110 neuron-layer combinations. The more complex a network, the smaller the in-sample MSE becomes, but the larger the out-of-sample MSE becomes.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces MSE median across all 9 splits. For each network architecture, 50 neural networks were trained (55`500 models per time split). The nine colors illustrate how these models behave in the different time splits. Time periods with a strong trend (for example, the dark red line, split 7) lead to the same structure as already noted in the previous graphs. The more complex a network, the smaller the in-sample MSE becomes, but the larger it becomes out-of-sample.\relax }}{30}{figure.caption.50}\protected@file@percent }
\newlabel{fig:mse_median}{{19}{30}{MSE median across all 9 splits. For each network architecture, 50 neural networks were trained (55`500 models per time split). The nine colors illustrate how these models behave in the different time splits. Time periods with a strong trend (for example, the dark red line, split 7) lead to the same structure as already noted in the previous graphs. The more complex a network, the smaller the in-sample MSE becomes, but the larger it becomes out-of-sample.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Sharpe median across all 9 splits. Sharpe ratios increase in-sample (upper plot) with increasing complexity. Out-of-sample Sharpe ratios tend to decrease with increasing complexity. However, the effect is rather small here.\relax }}{31}{figure.caption.51}\protected@file@percent }
\newlabel{fig:sharpe_median}{{20}{31}{Sharpe median across all 9 splits. Sharpe ratios increase in-sample (upper plot) with increasing complexity. Out-of-sample Sharpe ratios tend to decrease with increasing complexity. However, the effect is rather small here.\relax }{figure.caption.51}{}}
\newlabel{mod-select}{{}{32}{3.2.4. Model Selection}{section*.52}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.4. Model Selection}{32}{section*.52}\protected@file@percent }
\newlabel{benchmark}{{}{32}{3.2.5. Benchmark}{section*.53}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.5. Benchmark}{32}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3.3. Trading Strategy}{33}{section*.54}\protected@file@percent }
\newlabel{trading-strat}{{}{33}{3.3. Trading Strategy}{section*.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces This flowchart illustrates an overview of the trading strategy applied in this section. From left to right: Trading signals are created from forecasts, XAI and GARCH and combined at the end.\relax }}{33}{figure.caption.55}\protected@file@percent }
\newlabel{fig:flowchart_trading}{{21}{33}{This flowchart illustrates an overview of the trading strategy applied in this section. From left to right: Trading signals are created from forecasts, XAI and GARCH and combined at the end.\relax }{figure.caption.55}{}}
\newlabel{trading_lpd_nn}{{}{34}{3.3.1. Trading with Neural Networks and LPD}{section*.56}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.1. Trading with Neural Networks and LPD}{34}{section*.56}\protected@file@percent }
\newlabel{neural-network}{{}{34}{3.3.2. Neural Network}{section*.57}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.2. Neural Network}{34}{section*.57}\protected@file@percent }
\newlabel{eq:net_decision1}{{16}{34}{3.3.2. Neural Network}{equation.0.16}{}}
\newlabel{lpd-signal}{{}{35}{3.3.3. LPD Signal}{section*.58}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.3. LPD Signal}{35}{section*.58}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces LPD Plot for illustration: Visualization of the above signal generation procedure. For the top LPD time series, the mean value and the upper and lower decision bands are plotted.\relax }}{35}{figure.caption.59}\protected@file@percent }
\newlabel{fig:lpd_explain}{{22}{35}{LPD Plot for illustration: Visualization of the above signal generation procedure. For the top LPD time series, the mean value and the upper and lower decision bands are plotted.\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces LPD Sum Plot for illustration: Sum of the LPDs in black. Red is the upper decision limit and the green is the lower one. Values below the green line lead to a signal = 1. Values above the red line lead to a signal = 0. Values between the red and the green line lead to a signal = 0.5.\relax }}{35}{figure.caption.60}\protected@file@percent }
\newlabel{fig:lpd_explain_sum}{{23}{35}{LPD Sum Plot for illustration: Sum of the LPDs in black. Red is the upper decision limit and the green is the lower one. Values below the green line lead to a signal = 1. Values above the red line lead to a signal = 0. Values between the red and the green line lead to a signal = 0.5.\relax }{figure.caption.60}{}}
\newlabel{eq:Ybar}{{17}{36}{3.3.3. LPD Signal}{equation.0.17}{}}
\newlabel{eq:sdY}{{18}{36}{3.3.3. LPD Signal}{equation.0.18}{}}
\newlabel{eq:count}{{19}{36}{3.3.3. LPD Signal}{equation.0.19}{}}
\newlabel{eq:net_decision2}{{20}{36}{3.3.3. LPD Signal}{equation.0.20}{}}
\newlabel{garch-signal}{{}{37}{3.3.4. GARCH Volatility Predictions}{section*.61}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.4. GARCH Volatility Predictions}{37}{section*.61}\protected@file@percent }
\newlabel{eq:vola_predict}{{21}{37}{3.3.4. GARCH Volatility Predictions}{equation.0.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Cumulative daily returns of two different GARCH trading strategies and a simple buy-and-hold strategy. The GARCH Signum strategy is based on the ARMA(1,1)-GARCH(1,1) prediction, while the GARCH Volatility strategy simply checks if the threshold is exceeded. The time periods where we quit the market is clearly visible as horizontal lines.\relax }}{38}{figure.caption.62}\protected@file@percent }
\newlabel{fig:garch_trading}{{24}{38}{Cumulative daily returns of two different GARCH trading strategies and a simple buy-and-hold strategy. The GARCH Signum strategy is based on the ARMA(1,1)-GARCH(1,1) prediction, while the GARCH Volatility strategy simply checks if the threshold is exceeded. The time periods where we quit the market is clearly visible as horizontal lines.\relax }{figure.caption.62}{}}
\newlabel{nn-and-lpd-estimation}{{}{39}{3.3.5. NN and LPD Estimation}{section*.63}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.5. NN and LPD Estimation}{39}{section*.63}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Visualization of different signaling combinations. The best performing rule here is buy and hold in green. It is closely followed by the light blue NN+LPD combination with a $\kappa $ of 0.2. The behavior of the individual rules is often very similar, few different decisions can influence the performance. Especially at the end of July, beginning of August the rules behave differently, but then usually behave very similarly.\relax }}{40}{figure.caption.64}\protected@file@percent }
\newlabel{fig:perf}{{25}{40}{Visualization of different signaling combinations. The best performing rule here is buy and hold in green. It is closely followed by the light blue NN+LPD combination with a $\kappa $ of 0.2. The behavior of the individual rules is often very similar, few different decisions can influence the performance. Especially at the end of July, beginning of August the rules behave differently, but then usually behave very similarly.\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Sharpe ratios of different signaling combinations. Based on the performance measure Sharpe ratios, the LPDs lead to a better performance. According to the Sharpe ratio, the performance of the NN+LPD signaling rule with a $\kappa $ of 0.2, is better than Buy and Hold.\relax }}{41}{figure.caption.65}\protected@file@percent }
\newlabel{fig:sharpe}{{26}{41}{Sharpe ratios of different signaling combinations. Based on the performance measure Sharpe ratios, the LPDs lead to a better performance. According to the Sharpe ratio, the performance of the NN+LPD signaling rule with a $\kappa $ of 0.2, is better than Buy and Hold.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Sharpe ratios from the same different signaling combinations as in the last two graphs, but each for the 9 different time splits. Certain combinations have very similar decisions and thus lead here also to similar trajectories of the Sharpe ratios.\relax }}{42}{figure.caption.66}\protected@file@percent }
\newlabel{fig:batch}{{27}{42}{Sharpe ratios from the same different signaling combinations as in the last two graphs, but each for the 9 different time splits. Certain combinations have very similar decisions and thus lead here also to similar trajectories of the Sharpe ratios.\relax }{figure.caption.66}{}}
\newlabel{adding-ether}{{}{42}{3.3.6. Adding Ether}{section*.67}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.6. Adding Ether}{42}{section*.67}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces In addition to LPD+NN combinations with different beta, combinations with ETH are added here. The combination LPD+NN+ETH with a beta of 0.2, clearly performs best here. Overall, even 3 combinations are better than buy and hold.\relax }}{43}{figure.caption.68}\protected@file@percent }
\newlabel{fig:with_eth}{{28}{43}{In addition to LPD+NN combinations with different beta, combinations with ETH are added here. The combination LPD+NN+ETH with a beta of 0.2, clearly performs best here. Overall, even 3 combinations are better than buy and hold.\relax }{figure.caption.68}{}}
\newlabel{eq:logreturns_transforming}{{22}{43}{3.3.6. Adding Ether}{equation.0.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Visualization of the best performing signaling and the benchmark buy and hold. One can clearly see that as of August 2020, the purple signaling strongly contrasts with the green benchmark. There are some decisions which lead to a better performance than simple buy and hold.\relax }}{44}{figure.caption.69}\protected@file@percent }
\newlabel{fig:with_eth_sole}{{29}{44}{Visualization of the best performing signaling and the benchmark buy and hold. One can clearly see that as of August 2020, the purple signaling strongly contrasts with the green benchmark. There are some decisions which lead to a better performance than simple buy and hold.\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{4. Results}{44}{section*.70}\protected@file@percent }
\newlabel{results}{{}{44}{4. Results}{section*.70}{}}
\@writefile{toc}{\contentsline {subsection}{5. Conclusion}{45}{section*.71}\protected@file@percent }
\newlabel{conclusion}{{}{45}{5. Conclusion}{section*.71}{}}
\@writefile{toc}{\contentsline {subsection}{References}{46}{section*.72}\protected@file@percent }
\newlabel{references}{{}{46}{References}{section*.72}{}}
\@writefile{toc}{\contentsline {subsection}{Attachment}{48}{section*.73}\protected@file@percent }
\newlabel{attachment}{{}{48}{Attachment}{section*.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces MSE mean over all 9 splits with all 3 layers.\relax }}{48}{figure.caption.74}\protected@file@percent }
\newlabel{fig:meanmean1}{{30}{48}{MSE mean over all 9 splits with all 3 layers.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces MSE mean over all 9 splits with only 2 layers.\relax }}{48}{figure.caption.75}\protected@file@percent }
\newlabel{fig:meanmean2}{{31}{48}{MSE mean over all 9 splits with only 2 layers.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Split 1.\relax }}{49}{figure.caption.76}\protected@file@percent }
\newlabel{fig:price1}{{32}{49}{Split 1.\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Split 2.\relax }}{49}{figure.caption.77}\protected@file@percent }
\newlabel{fig:price2}{{33}{49}{Split 2.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Split 3.\relax }}{49}{figure.caption.78}\protected@file@percent }
\newlabel{fig:price3}{{34}{49}{Split 3.\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Split 4.\relax }}{50}{figure.caption.79}\protected@file@percent }
\newlabel{fig:price4}{{35}{50}{Split 4.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Split 5.\relax }}{50}{figure.caption.80}\protected@file@percent }
\newlabel{fig:price5}{{36}{50}{Split 5.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Split 6.\relax }}{50}{figure.caption.81}\protected@file@percent }
\newlabel{fig:price6}{{37}{50}{Split 6.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Split 7.\relax }}{51}{figure.caption.82}\protected@file@percent }
\newlabel{fig:price7}{{38}{51}{Split 7.\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Split 8.\relax }}{51}{figure.caption.83}\protected@file@percent }
\newlabel{fig:price8}{{39}{51}{Split 8.\relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Split 9.\relax }}{51}{figure.caption.84}\protected@file@percent }
\newlabel{fig:price9}{{40}{51}{Split 9.\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces One-step-ahead forecasts of volatility using rolling window of size 365. Refitting model after every months.\relax }}{52}{figure.caption.85}\protected@file@percent }
\newlabel{fig:vola_forecasts}{{41}{52}{One-step-ahead forecasts of volatility using rolling window of size 365. Refitting model after every months.\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Split 9.\relax }}{53}{figure.caption.86}\protected@file@percent }
\newlabel{fig:all2_05}{{42}{53}{Split 9.\relax }{figure.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Split 9.\relax }}{53}{figure.caption.87}\protected@file@percent }
\newlabel{fig:all2}{{43}{53}{Split 9.\relax }{figure.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Split 9.\relax }}{54}{figure.caption.88}\protected@file@percent }
\newlabel{fig:all1_5_05}{{44}{54}{Split 9.\relax }{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Split 9.\relax }}{54}{figure.caption.89}\protected@file@percent }
\newlabel{fig:all1_5}{{45}{54}{Split 9.\relax }{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Split 9.\relax }}{55}{figure.caption.90}\protected@file@percent }
\newlabel{fig:all1_05}{{46}{55}{Split 9.\relax }{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces LPD of BTC.\relax }}{55}{figure.caption.91}\protected@file@percent }
\newlabel{fig:xai_plot2}{{47}{55}{LPD of BTC.\relax }{figure.caption.91}{}}
\gdef \@abspage@last{61}
