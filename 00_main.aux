\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {subsection}{Abstract}{}{section*.2}\protected@file@percent }
\newlabel{abstract}{{}{}{Abstract}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Zusammenfassung}{}{section*.3}\protected@file@percent }
\newlabel{zusammenfassung}{{}{}{Zusammenfassung}{section*.3}{}}
\@writefile{toc}{\contentsline {subsection}{1. Introduction}{1}{section*.4}\protected@file@percent }
\newlabel{introduction}{{}{1}{1. Introduction}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{2. Theory}{2}{section*.5}\protected@file@percent }
\newlabel{theory}{{}{2}{2. Theory}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.1. Neural network}{2}{section*.6}\protected@file@percent }
\newlabel{neural_network}{{}{2}{2.1. Neural network}{section*.6}{}}
\newlabel{perceptron}{{}{2}{2.1.1. Perceptron}{section*.7}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.1. Perceptron}{2}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a perceptron. The input data is combined with the weights and added up to a weighted sum. After adding an error term (bias), the activation function is applied to the total sum to produce the output.\relax }}{3}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron_schema}{{1}{3}{Schematic diagram of a perceptron. The input data is combined with the weights and added up to a weighted sum. After adding an error term (bias), the activation function is applied to the total sum to produce the output.\relax }{figure.caption.8}{}}
\newlabel{eq:perceptron}{{1}{3}{2.1.1. Perceptron}{equation.0.1}{}}
\newlabel{backprogation_algorithm}{{}{4}{2.1.2. Backpropagation Algorithm}{section*.9}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.2. Backpropagation Algorithm}{4}{section*.9}\protected@file@percent }
\newlabel{eq:sigmoid_logistic}{{2}{4}{2.1.2. Backpropagation Algorithm}{equation.0.2}{}}
\newlabel{eq:sigmoid_tanh}{{3}{4}{2.1.2. Backpropagation Algorithm}{equation.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two common non-linear sigmoid activation functions: In black, the logistic function, which returns values between 0 and 1. In red the hyperbolic tangent function, which returns values between -1 and 1.\relax }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sigmoid}{{2}{4}{Two common non-linear sigmoid activation functions: In black, the logistic function, which returns values between 0 and 1. In red the hyperbolic tangent function, which returns values between -1 and 1.\relax }{figure.caption.10}{}}
\newlabel{eq:mse}{{4}{5}{2.1.2. Backpropagation Algorithm}{equation.0.4}{}}
\newlabel{eq:gradient_descent}{{5}{5}{2.1.2. Backpropagation Algorithm}{equation.0.5}{}}
\newlabel{eq:weight_adj}{{6}{5}{2.1.2. Backpropagation Algorithm}{equation.0.6}{}}
\newlabel{MLP}{{}{6}{2.1.3. Multilayer Perceptron}{section*.11}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.3. Multilayer Perceptron}{6}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of a multilayer perceptron. On the left, you see the input layer, on the right the output layer. All layers in between are described as hidden layers. If there is more than one hidden layer, it is referred to as a deep learning neural network.\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mlp_schema}{{3}{6}{Schematic diagram of a multilayer perceptron. On the left, you see the input layer, on the right the output layer. All layers in between are described as hidden layers. If there is more than one hidden layer, it is referred to as a deep learning neural network.\relax }{figure.caption.12}{}}
\newlabel{challenges}{{}{7}{2.1.4. Challenges}{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{2.1.4. Challenges}{7}{section*.13}\protected@file@percent }
\newlabel{overfitting}{{}{7}{2.1.4.1. Overfitting}{section*.14}{}}
\@writefile{toc}{\contentsline {subparagraph}{2.1.4.1. Overfitting}{7}{section*.14}\protected@file@percent }
\newlabel{vanishing_gradient}{{}{7}{2.1.4.2. Vanishing Gradient Problem}{section*.15}{}}
\@writefile{toc}{\contentsline {subparagraph}{2.1.4.2. Vanishing Gradient Problem}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2.2. Explainable Artificial Intelligence}{8}{section*.16}\protected@file@percent }
\newlabel{xai}{{}{8}{2.2. Explainable Artificial Intelligence}{section*.16}{}}
\newlabel{classic}{{}{8}{2.2.1. Classic Approach}{section*.17}{}}
\@writefile{toc}{\contentsline {paragraph}{2.2.1. Classic Approach}{8}{section*.17}\protected@file@percent }
\newlabel{xai_finance}{{}{9}{2.2.2. Explainability for Financial Time Series}{section*.18}{}}
\@writefile{toc}{\contentsline {paragraph}{2.2.2. Explainability for Financial Time Series}{9}{section*.18}\protected@file@percent }
\newlabel{eq:lm1}{{7}{9}{2.2.2. Explainability for Financial Time Series}{equation.0.7}{}}
\newlabel{eq:lm2}{{8}{9}{2.2.2. Explainability for Financial Time Series}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Autocorrelation function of BTC/USD. In red and orange you can see significant positive ACF's at lags 6 and 10. These signal some importance for model considerations. Between 6 and 10, the ACF's are negative and thus would have a negative effect on a model. For further considerations, all lags from 6 to 10 are included.\relax }}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig:xai_acf}{{4}{9}{Autocorrelation function of BTC/USD. In red and orange you can see significant positive ACF's at lags 6 and 10. These signal some importance for model considerations. Between 6 and 10, the ACF's are negative and thus would have a negative effect on a model. For further considerations, all lags from 6 to 10 are included.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Coefficients 6 (red) and 10 (orange) have the greatest positive impact on the model. In between, the coefficients are negative.\relax }}{10}{table.caption.20}\protected@file@percent }
\newlabel{tab:lm_tab}{{1}{10}{Coefficients 6 (red) and 10 (orange) have the greatest positive impact on the model. In between, the coefficients are negative.\relax }{table.caption.20}{}}
\newlabel{eq:xai_partial}{{9}{10}{2.2.2. Explainability for Financial Time Series}{equation.0.9}{}}
\newlabel{eq:xai_fit}{{10}{10}{2.2.2. Explainability for Financial Time Series}{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Upper panel: LPD's of BTC's log returns. Only lags between 6 and 10 are highlighted in color. Lag 6 (red) and 10 (orange) have the largest positive impact. 7, 8 and 9 have a negative impact. Lower panel: Logarithmic returns of BTC. Phases with high volatility are visible in both panels, as the input data (log returns) affect the LPD's.\relax }}{11}{figure.caption.21}\protected@file@percent }
\newlabel{fig:xai_plot1}{{5}{11}{Upper panel: LPD's of BTC's log returns. Only lags between 6 and 10 are highlighted in color. Lag 6 (red) and 10 (orange) have the largest positive impact. 7, 8 and 9 have a negative impact. Lower panel: Logarithmic returns of BTC. Phases with high volatility are visible in both panels, as the input data (log returns) affect the LPD's.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison between linear regression (LR) and linear parameter data (LPD) coefficients. Again, lags 6 and 10 have the largest positive effect in the respective model. Also 7, 8 and 9 all have a negative sign.\relax }}{12}{table.caption.22}\protected@file@percent }
\newlabel{tab:lpd_tab}{{2}{12}{Comparison between linear regression (LR) and linear parameter data (LPD) coefficients. Again, lags 6 and 10 have the largest positive effect in the respective model. Also 7, 8 and 9 all have a negative sign.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.3. Model Comparison}{13}{section*.23}\protected@file@percent }
\newlabel{model_comparison}{{}{13}{2.3. Model Comparison}{section*.23}{}}
\newlabel{sharpe_ratio}{{}{13}{2.3.1. Sharpe Ratio}{section*.24}{}}
\@writefile{toc}{\contentsline {paragraph}{2.3.1. Sharpe Ratio}{13}{section*.24}\protected@file@percent }
\newlabel{eq:Sharpe}{{11}{13}{2.3.1. Sharpe Ratio}{equation.0.11}{}}
\newlabel{MSE}{{}{13}{2.3.2. Mean Squared Error (MSE)}{section*.25}{}}
\@writefile{toc}{\contentsline {paragraph}{2.3.2. Mean Squared Error (MSE)}{13}{section*.25}\protected@file@percent }
\newlabel{eq:MSE}{{12}{13}{2.3.2. Mean Squared Error (MSE)}{equation.0.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{2.4. Bitcoin}{14}{section*.26}\protected@file@percent }
\newlabel{bitcoin}{{}{14}{2.4. Bitcoin}{section*.26}{}}
\newlabel{historical_analysis}{{}{14}{2.4.1. Historical Analysis}{section*.27}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.1. Historical Analysis}{14}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Price development of BTC in USD from fall 2014 to spring 2021. Events worth mentioning are marked with letters A to E and described in the text above.\relax }}{15}{figure.caption.28}\protected@file@percent }
\newlabel{fig:price_btc}{{6}{15}{Price development of BTC in USD from fall 2014 to spring 2021. Events worth mentioning are marked with letters A to E and described in the text above.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Logarithmic transformation of the price development of the upper chart. In this representation, the relative price changes are better visible. Thus, the events marked with the letters can be better compared with each other. In contrast to the upper chart, here you can see not only the extreme price spikes, but also the rather small price movements.\relax }}{16}{figure.caption.29}\protected@file@percent }
\newlabel{fig:logprice_btc}{{7}{16}{Logarithmic transformation of the price development of the upper chart. In this representation, the relative price changes are better visible. Thus, the events marked with the letters can be better compared with each other. In contrast to the upper chart, here you can see not only the extreme price spikes, but also the rather small price movements.\relax }{figure.caption.29}{}}
\newlabel{Bitcoin_tec}{{}{16}{2.4.2. Cryptocurrencies and Bitcoin Technology}{section*.30}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.2. Cryptocurrencies and Bitcoin Technology}{16}{section*.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Schematic diagram of a blockchain. In black are the previous blocks. This is followed by a fork in red and green. The red block is wrong and is not accepted, because the green path is longer and only this one is trusted.\relax }}{17}{figure.caption.31}\protected@file@percent }
\newlabel{fig:blockchain}{{8}{17}{Schematic diagram of a blockchain. In black are the previous blocks. This is followed by a fork in red and green. The red block is wrong and is not accepted, because the green path is longer and only this one is trusted.\relax }{figure.caption.31}{}}
\newlabel{Bitcoin_valuation}{{}{18}{2.4.3. Valuation and Digital Gold}{section*.32}{}}
\@writefile{toc}{\contentsline {paragraph}{2.4.3. Valuation and Digital Gold}{18}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{3. Methodology}{19}{section*.33}\protected@file@percent }
\newlabel{methodology}{{}{19}{3. Methodology}{section*.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces This flowchart illustrates an overview of the individual intermediate steps that are covered in the methodology section. After the data is explored, the details for the network archtecture are examined and compared quantitatively. Finally, aspects of explainable artificial intelligence (XAI) and traditional time series analysis are applied for the implementation of the trading strategy.\relax }}{19}{figure.caption.34}\protected@file@percent }
\newlabel{fig:flowchart_overview}{{9}{19}{This flowchart illustrates an overview of the individual intermediate steps that are covered in the methodology section. After the data is explored, the details for the network archtecture are examined and compared quantitatively. Finally, aspects of explainable artificial intelligence (XAI) and traditional time series analysis are applied for the implementation of the trading strategy.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.1. Data Exploration}{20}{section*.35}\protected@file@percent }
\newlabel{data_exploration}{{}{20}{3.1. Data Exploration}{section*.35}{}}
\newlabel{eq:logreturn}{{14}{20}{3.1. Data Exploration}{equation.0.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Logarithmic returns of BTC/USD. Data are available from fall 2014 to spring 2021. The volatility clusters typical for time series are apparent. In these phases, the log returns fluctuate strongly.\relax }}{20}{figure.caption.36}\protected@file@percent }
\newlabel{fig:log_ret}{{10}{20}{Logarithmic returns of BTC/USD. Data are available from fall 2014 to spring 2021. The volatility clusters typical for time series are apparent. In these phases, the log returns fluctuate strongly.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Autocorrelation function of BTC log returns during the entire time period from fall 2014 to spring 2021. Lag 6 and 10 are significant (values exceed the blue dotted line) and thus have an impact on possible models.\relax }}{21}{figure.caption.37}\protected@file@percent }
\newlabel{fig:acf_log_ret}{{11}{21}{Autocorrelation function of BTC log returns during the entire time period from fall 2014 to spring 2021. Lag 6 and 10 are significant (values exceed the blue dotted line) and thus have an impact on possible models.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces This histogram of BTC log returns illustrates how well it fits to different distributions. The blue t-Student distribution would fit the data better in the tails, while the yellow normal distribution would disperse a little better around the expected value. The distribution should be taken with a grain of salt, as the time dependence of the volatility is not taken into account.\relax }}{21}{figure.caption.38}\protected@file@percent }
\newlabel{fig:histogramm_logreturns}{{12}{21}{This histogram of BTC log returns illustrates how well it fits to different distributions. The blue t-Student distribution would fit the data better in the tails, while the yellow normal distribution would disperse a little better around the expected value. The distribution should be taken with a grain of salt, as the time dependence of the volatility is not taken into account.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.2. Network Architecture}{22}{section*.39}\protected@file@percent }
\newlabel{networkarchitecture}{{}{22}{3.2. Network Architecture}{section*.39}{}}
\newlabel{eq:comb}{{15}{22}{3.2. Network Architecture}{equation.0.15}{}}
\newlabel{test_train}{{}{23}{3.2.1. Defining Train and Test Samples}{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.1. Defining Train and Test Samples}{23}{section*.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Nine different in- and out-of-sample splits. The 6-month training phase (in-sample) is indicated in blue and the 1-month test phase (out-of-sample) in green. The splits capture different phases of the time series. Both very turbulent phases (split 1) and rather calm phases (split 5) can be recorded.\relax }}{23}{figure.caption.41}\protected@file@percent }
\newlabel{fig:test_train}{{13}{23}{Nine different in- and out-of-sample splits. The 6-month training phase (in-sample) is indicated in blue and the 1-month test phase (out-of-sample) in green. The splits capture different phases of the time series. Both very turbulent phases (split 1) and rather calm phases (split 5) can be recorded.\relax }{figure.caption.41}{}}
\newlabel{input-layer}{{}{24}{3.2.2. Defining Input-Layer}{section*.42}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.2. Defining Input-Layer}{24}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Autocorrelation function of our chosen time period. One can clearly see an increased influence of lags 4 and 7, not as in the ACF's of the entire data set with significant values at 6 and 10. Only late at lag 18 does the ACF show a significant value again.\relax }}{24}{figure.caption.43}\protected@file@percent }
\newlabel{fig:test_train_acf}{{14}{24}{Autocorrelation function of our chosen time period. One can clearly see an increased influence of lags 4 and 7, not as in the ACF's of the entire data set with significant values at 6 and 10. Only late at lag 18 does the ACF show a significant value again.\relax }{figure.caption.43}{}}
\newlabel{train-nn}{{}{25}{3.2.3. Neural Network Training}{section*.44}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.3. Neural Network Training}{25}{section*.44}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Adapted data of the 5th test/train split for training neural networks with the R package neuralnet. Lag0 corresponds to the original data while the rest are simply the corresponding lag of the original data. The color-coded values show that these are simply shifted values.\relax }}{25}{table.caption.45}\protected@file@percent }
\newlabel{tab:data_tab}{{3}{25}{Adapted data of the 5th test/train split for training neural networks with the R package neuralnet. Lag0 corresponds to the original data while the rest are simply the corresponding lag of the original data. The color-coded values show that these are simply shifted values.\relax }{table.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Scaled data from the 5th test/train split. This is the same data as in the previous table, only here the data is scaled between 0 and 1.\relax }}{26}{table.caption.46}\protected@file@percent }
\newlabel{tab:data_tab_scaled}{{4}{26}{Scaled data from the 5th test/train split. This is the same data as in the previous table, only here the data is scaled between 0 and 1.\relax }{table.caption.46}{}}
\newlabel{evaluate_nn}{{}{27}{3.2.4. Evaluating Network Architecture}{section*.47}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.4. Evaluating Network Architecture}{27}{section*.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces In- and out-of-sample MSE of the 5th time split. Only the networks with only one layer are shown here. Simplest network has 1 neuron (leftmost) and the most complex one has 10 neurons (rightmost), therefore we have the MSE's of 10 different neural networks. The in-sample MSE's tend to decrease, while the out-of-sample MSE's move the opposite direction.\relax }}{27}{figure.caption.48}\protected@file@percent }
\newlabel{fig:mse_plot1}{{15}{27}{In- and out-of-sample MSE of the 5th time split. Only the networks with only one layer are shown here. Simplest network has 1 neuron (leftmost) and the most complex one has 10 neurons (rightmost), therefore we have the MSE's of 10 different neural networks. The in-sample MSE's tend to decrease, while the out-of-sample MSE's move the opposite direction.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces In- and out-of-sample MSE of the 5th split. Shown here are only the networks with two layers. The simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 100 different neural networks. As in the previous graph, in-sample MSE's tend to decrease while out-of-sample MSE's tend to increase.\relax }}{28}{figure.caption.49}\protected@file@percent }
\newlabel{fig:mse_plot2}{{16}{28}{In- and out-of-sample MSE of the 5th split. Shown here are only the networks with two layers. The simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 100 different neural networks. As in the previous graph, in-sample MSE's tend to decrease while out-of-sample MSE's tend to increase.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces In- and out-of-sample MSE of the 5th split. Shown here are only the networks with 3 layers. Simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 1000 different neural networks. Same pattern as for 1 layer and 2 layers: The in-sample the MSE's decreases, while the out-of-sample MSE's increase.\relax }}{29}{figure.caption.50}\protected@file@percent }
\newlabel{fig:mse_plot3}{{17}{29}{In- and out-of-sample MSE of the 5th split. Shown here are only the networks with 3 layers. Simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 1000 different neural networks. Same pattern as for 1 layer and 2 layers: The in-sample the MSE's decreases, while the out-of-sample MSE's increase.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces In- and out-of-sample MSE of the 5th split. The 3 previous graphs are summarized here and represents an overview of all 3 layers, which results in a total of 1110 neuron-layer combinations. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.\relax }}{30}{figure.caption.51}\protected@file@percent }
\newlabel{fig:mse_plot5}{{18}{30}{In- and out-of-sample MSE of the 5th split. The 3 previous graphs are summarized here and represents an overview of all 3 layers, which results in a total of 1110 neuron-layer combinations. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces We take the median of all MSE's for a given network architecture. The 9 splits are visualized by different colors. For each network architecture, 50 neural networks were trained (55`500 models per time split). The nine colors illustrate how these models behave in the different time splits. Time periods with a strong trend (for example, the dark red line, split 7) lead to the same structure as already noted in the previous graphs. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.\relax }}{31}{figure.caption.52}\protected@file@percent }
\newlabel{fig:mse_median}{{19}{31}{We take the median of all MSE's for a given network architecture. The 9 splits are visualized by different colors. For each network architecture, 50 neural networks were trained (55`500 models per time split). The nine colors illustrate how these models behave in the different time splits. Time periods with a strong trend (for example, the dark red line, split 7) lead to the same structure as already noted in the previous graphs. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces We take the median of all Sharpe ratios for a given network architecture. The 9 splits are visualized by different colors. The in-sample Sharpe ratios increase (upper plot) with increasing complexity. The out-of-sample Sharpe ratios tend to decrease with increasing complexity. In comparison to the previous plots using the MSE, the effect is much smaller here.\relax }}{32}{figure.caption.53}\protected@file@percent }
\newlabel{fig:sharpe_median}{{20}{32}{We take the median of all Sharpe ratios for a given network architecture. The 9 splits are visualized by different colors. The in-sample Sharpe ratios increase (upper plot) with increasing complexity. The out-of-sample Sharpe ratios tend to decrease with increasing complexity. In comparison to the previous plots using the MSE, the effect is much smaller here.\relax }{figure.caption.53}{}}
\newlabel{mod-select}{{}{33}{3.2.5. Model Selection}{section*.54}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.5. Model Selection}{33}{section*.54}\protected@file@percent }
\newlabel{benchmark}{{}{33}{3.2.6. Benchmark}{section*.55}{}}
\@writefile{toc}{\contentsline {paragraph}{3.2.6. Benchmark}{33}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3.3. Trading Strategy}{34}{section*.56}\protected@file@percent }
\newlabel{trading-strat}{{}{34}{3.3. Trading Strategy}{section*.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces This flowchart illustrates an overview of the trading strategy applied in this section. After finding the best network architecture for accurate forecasts of log returns, we check the stability of the neural network with XAI. The GARCH-model addresses the phenomenon of volatility clusters. Finally, all these information are combined in a trading strategy in order to be compared with our benchmark consisting of buy-and-hold.\relax }}{34}{figure.caption.57}\protected@file@percent }
\newlabel{fig:flowchart_trading}{{21}{34}{This flowchart illustrates an overview of the trading strategy applied in this section. After finding the best network architecture for accurate forecasts of log returns, we check the stability of the neural network with XAI. The GARCH-model addresses the phenomenon of volatility clusters. Finally, all these information are combined in a trading strategy in order to be compared with our benchmark consisting of buy-and-hold.\relax }{figure.caption.57}{}}
\newlabel{trading_lpd_nn}{{}{35}{3.3.1. Trading with Neural Networks and LPD}{section*.58}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.1. Trading with Neural Networks and LPD}{35}{section*.58}\protected@file@percent }
\newlabel{network}{{}{35}{3.3.1.1. Neural Network}{section*.59}{}}
\@writefile{toc}{\contentsline {subparagraph}{3.3.1.1. Neural Network}{35}{section*.59}\protected@file@percent }
\newlabel{eq:net_decision1}{{16}{35}{3.3.1.1. Neural Network}{equation.0.16}{}}
\newlabel{lpd-signal}{{}{36}{3.3.1.2. LPD Signal}{section*.60}{}}
\@writefile{toc}{\contentsline {subparagraph}{3.3.1.2. LPD Signal}{36}{section*.60}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The LPD is used to generate the trading signal. Higher volatility indicates that the neural network may provide unreliable outputs. For the top LPD time series, the mean value and the upper and lower decision bands are plotted. Each time this LPD exceeds the upper or lower decision band, a signal is realized for this lag. In the most extreme phases it can happen that several lags exceed the decision bands and thus generate several signals. The number of all exceedances will be considered in the next plot in order to generate the XAI trading signal.\relax }}{36}{figure.caption.61}\protected@file@percent }
\newlabel{fig:lpd_explain}{{22}{36}{The LPD is used to generate the trading signal. Higher volatility indicates that the neural network may provide unreliable outputs. For the top LPD time series, the mean value and the upper and lower decision bands are plotted. Each time this LPD exceeds the upper or lower decision band, a signal is realized for this lag. In the most extreme phases it can happen that several lags exceed the decision bands and thus generate several signals. The number of all exceedances will be considered in the next plot in order to generate the XAI trading signal.\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The black line shows how many lags exceed the threshold defined above at one point in time. The more extreme lags are counted, the more unstable the neural network is. Red is the upper decision limit and the green is the lower one. Values below the green line indicate realiability and lead to a signal = 1. Values above the red line indicate instability and lead to a signal = 0. Values between the red and the green line lead to a signal = 0.5.\relax }}{37}{figure.caption.62}\protected@file@percent }
\newlabel{fig:lpd_explain_sum}{{23}{37}{The black line shows how many lags exceed the threshold defined above at one point in time. The more extreme lags are counted, the more unstable the neural network is. Red is the upper decision limit and the green is the lower one. Values below the green line indicate realiability and lead to a signal = 1. Values above the red line indicate instability and lead to a signal = 0. Values between the red and the green line lead to a signal = 0.5.\relax }{figure.caption.62}{}}
\newlabel{eq:Ybar}{{17}{38}{3.3.1.2. LPD Signal}{equation.0.17}{}}
\newlabel{eq:sdY}{{18}{38}{3.3.1.2. LPD Signal}{equation.0.18}{}}
\newlabel{eq:count}{{19}{38}{3.3.1.2. LPD Signal}{equation.0.19}{}}
\newlabel{eq:net_decision2}{{20}{38}{3.3.1.2. LPD Signal}{equation.0.20}{}}
\newlabel{garch-signal}{{}{39}{3.3.2. GARCH Volatility Predictions}{section*.63}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.2. GARCH Volatility Predictions}{39}{section*.63}\protected@file@percent }
\newlabel{eq:vola_predict}{{21}{39}{3.3.2. GARCH Volatility Predictions}{equation.0.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Cumulative daily returns of two different GARCH trading strategies and a simple buy-and-hold strategy. The GARCH signum strategy is based on the ARMA(1,1)-GARCH(1,1) prediction, while the GARCH volatility strategy simply checks if the threshold is exceeded. The time periods where we quit the market is clearly visible as horizontal lines.\relax }}{40}{figure.caption.64}\protected@file@percent }
\newlabel{fig:garch_trading}{{24}{40}{Cumulative daily returns of two different GARCH trading strategies and a simple buy-and-hold strategy. The GARCH signum strategy is based on the ARMA(1,1)-GARCH(1,1) prediction, while the GARCH volatility strategy simply checks if the threshold is exceeded. The time periods where we quit the market is clearly visible as horizontal lines.\relax }{figure.caption.64}{}}
\newlabel{neural-network-and-lpd-trading}{{}{41}{3.3.3. Neural Network and LPD Trading}{section*.65}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.3. Neural Network and LPD Trading}{41}{section*.65}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Different combinations of trading signals are compared to each other. The best performing one is buy-and-hold (green). It is closely followed by the light blue NN+LPD combination with a $\kappa $ of 0.2. The behavior of the individual rules is often very similar, few different decisions can influence the performance. Especially at the end of July, beginning of August the rules behave differently, but then usually behave very similarly.\relax }}{42}{figure.caption.66}\protected@file@percent }
\newlabel{fig:perf}{{25}{42}{Different combinations of trading signals are compared to each other. The best performing one is buy-and-hold (green). It is closely followed by the light blue NN+LPD combination with a $\kappa $ of 0.2. The behavior of the individual rules is often very similar, few different decisions can influence the performance. Especially at the end of July, beginning of August the rules behave differently, but then usually behave very similarly.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Sharpe ratios of different signaling combinations. Based on the performance measure Sharpe ratios, the LPDs lead to a better performance. According to the Sharpe ratio, the performance of the NN+LPD signaling rule with a $\kappa $ of 0.2, is better than buy-and-hold.\relax }}{43}{figure.caption.67}\protected@file@percent }
\newlabel{fig:sharpe}{{26}{43}{Sharpe ratios of different signaling combinations. Based on the performance measure Sharpe ratios, the LPDs lead to a better performance. According to the Sharpe ratio, the performance of the NN+LPD signaling rule with a $\kappa $ of 0.2, is better than buy-and-hold.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Sharpe ratios from the same different signaling combinations as in the last two graphs, but each for the 9 different time splits. Certain combinations have very similar decisions and thus lead here also to similar trajectories of the Sharpe ratios.\relax }}{43}{figure.caption.68}\protected@file@percent }
\newlabel{fig:batch}{{27}{43}{Sharpe ratios from the same different signaling combinations as in the last two graphs, but each for the 9 different time splits. Certain combinations have very similar decisions and thus lead here also to similar trajectories of the Sharpe ratios.\relax }{figure.caption.68}{}}
\newlabel{adding-ether}{{}{44}{3.3.3. Adding Ether}{section*.69}{}}
\@writefile{toc}{\contentsline {paragraph}{3.3.3. Adding Ether}{44}{section*.69}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces As described in this section, we investigate whether the addition of an alternative investment can lead to better performance. The trading signals with different kappas resulting from the neural networks and LPD (LPD+NN) are supplemented with occasional investments in ETH. The combination LPD+NN+ETH with a kappa of 0.2, clearly performs best here. Overall, even 3 combinations are better than buy-and-hold.\relax }}{44}{figure.caption.70}\protected@file@percent }
\newlabel{fig:with_eth}{{28}{44}{As described in this section, we investigate whether the addition of an alternative investment can lead to better performance. The trading signals with different kappas resulting from the neural networks and LPD (LPD+NN) are supplemented with occasional investments in ETH. The combination LPD+NN+ETH with a kappa of 0.2, clearly performs best here. Overall, even 3 combinations are better than buy-and-hold.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Visualization of the best performing signaling and the benchmark buy-and-hold. One can clearly see that as of August 2020, the purple signaling strongly contrasts with the green benchmark. There are some decisions which lead to a better performance than simple buy-and-hold.\relax }}{45}{figure.caption.71}\protected@file@percent }
\newlabel{fig:with_eth_sole}{{29}{45}{Visualization of the best performing signaling and the benchmark buy-and-hold. One can clearly see that as of August 2020, the purple signaling strongly contrasts with the green benchmark. There are some decisions which lead to a better performance than simple buy-and-hold.\relax }{figure.caption.71}{}}
\newlabel{eq:logreturns_transforming}{{22}{45}{3.3.3. Adding Ether}{equation.0.22}{}}
\@writefile{toc}{\contentsline {subsection}{4. Results}{46}{section*.72}\protected@file@percent }
\newlabel{results}{{}{46}{4. Results}{section*.72}{}}
\@writefile{toc}{\contentsline {subsection}{5. Conclusion}{47}{section*.73}\protected@file@percent }
\newlabel{conclusion}{{}{47}{5. Conclusion}{section*.73}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.1. Outlook}{48}{section*.74}\protected@file@percent }
\newlabel{outlook}{{}{48}{5.1. Outlook}{section*.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Data collected at the end of the writing phase of this thesis\relax }}{48}{figure.caption.75}\protected@file@percent }
\newlabel{fig:with_eth_sole_outlook}{{30}{48}{Data collected at the end of the writing phase of this thesis\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {subsection}{References}{50}{section*.76}\protected@file@percent }
\newlabel{references}{{}{50}{References}{section*.76}{}}
\@writefile{toc}{\contentsline {subsection}{Attachment}{52}{section*.77}\protected@file@percent }
\newlabel{attachement}{{}{52}{Attachment}{section*.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces MSE mean over all 9 splits with all 3 layers.\relax }}{52}{figure.caption.78}\protected@file@percent }
\newlabel{fig:meanmean1}{{31}{52}{MSE mean over all 9 splits with all 3 layers.\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces MSE mean over all 9 splits with only 2 layers.\relax }}{52}{figure.caption.79}\protected@file@percent }
\newlabel{fig:meanmean2}{{32}{52}{MSE mean over all 9 splits with only 2 layers.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Split 1.\relax }}{53}{figure.caption.80}\protected@file@percent }
\newlabel{fig:price1}{{33}{53}{Split 1.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Split 2.\relax }}{53}{figure.caption.81}\protected@file@percent }
\newlabel{fig:price2}{{34}{53}{Split 2.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Split 3.\relax }}{53}{figure.caption.82}\protected@file@percent }
\newlabel{fig:price3}{{35}{53}{Split 3.\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Split 4.\relax }}{54}{figure.caption.83}\protected@file@percent }
\newlabel{fig:price4}{{36}{54}{Split 4.\relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Split 5.\relax }}{54}{figure.caption.84}\protected@file@percent }
\newlabel{fig:price5}{{37}{54}{Split 5.\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Split 6.\relax }}{54}{figure.caption.85}\protected@file@percent }
\newlabel{fig:price6}{{38}{54}{Split 6.\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Split 7.\relax }}{55}{figure.caption.86}\protected@file@percent }
\newlabel{fig:price7}{{39}{55}{Split 7.\relax }{figure.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Split 8.\relax }}{55}{figure.caption.87}\protected@file@percent }
\newlabel{fig:price8}{{40}{55}{Split 8.\relax }{figure.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Split 9.\relax }}{55}{figure.caption.88}\protected@file@percent }
\newlabel{fig:price9}{{41}{55}{Split 9.\relax }{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces One-step-ahead forecasts of volatility using rolling window of size 365. Refitting model after every months.\relax }}{56}{figure.caption.89}\protected@file@percent }
\newlabel{fig:vola_forecasts}{{42}{56}{One-step-ahead forecasts of volatility using rolling window of size 365. Refitting model after every months.\relax }{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Split 9.\relax }}{57}{figure.caption.90}\protected@file@percent }
\newlabel{fig:all2_05}{{43}{57}{Split 9.\relax }{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Split 9.\relax }}{57}{figure.caption.91}\protected@file@percent }
\newlabel{fig:all2}{{44}{57}{Split 9.\relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Split 9.\relax }}{58}{figure.caption.92}\protected@file@percent }
\newlabel{fig:all1_5_05}{{45}{58}{Split 9.\relax }{figure.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Split 9.\relax }}{58}{figure.caption.93}\protected@file@percent }
\newlabel{fig:all1_5}{{46}{58}{Split 9.\relax }{figure.caption.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces Split 9.\relax }}{59}{figure.caption.94}\protected@file@percent }
\newlabel{fig:all1_05}{{47}{59}{Split 9.\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces LPD of BTC.\relax }}{59}{figure.caption.95}\protected@file@percent }
\newlabel{fig:xai_plot2}{{48}{59}{LPD of BTC.\relax }{figure.caption.95}{}}
\gdef \@abspage@last{65}
