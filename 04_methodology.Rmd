---
output:
  html_document:
    df_print: paged
geometry: margin = 1in
---

## 3. Methodology

The focus of this thesis can be divided into three areas. First, the aim is to find an optimal network architecture. This should perform well in the application area, in which the future log return of the Bitcoin is predicted on the basis of historical log returns. In a second step, it will be evaluated whether added value can be found with the help of XAI. Finally, we will focus on defining a trading strategy based on our findings. All considerations and findings will be presented in a quantitative way and compared with each other. Figure \ref{fig:flowchart_overview} helps to get an overview of the individual steps followed in this section.

&nbsp;

```{r flowchart_overview, fig.align='center', out.width='65%', fig.cap='This flowchart illustrates an overview of the individual intermediate steps that are covered in the methodology section. After the data is explored, the details for the network archtecture are examined and compared quantitatively. Finally, aspects of explainable artificial intelligence (XAI) and traditional time series analysis are applied for the implementation of the trading strategy.', echo=FALSE}
# knitr::include_graphics("images/flowchart_overview.png")
knitr::include_graphics("images/flowchart_overview2.png")
```

\newpage

### 3.1. Data Exploration {#data_exploration}

The data in this paper is accessed through the API of Yahoo Finance and is originally provided by the price-tracking website CoinMarketCap. We use the daily closing price of Bitcoin in USD with the ticker BTC-USD. As cryptocurrencies are traded 24/7, the closing price refers to the last price of the day evaluated at the last timestamp according to the Coordinated Universal Time (UTC).

In section [2.4](#bitcoin), the Bitcoin price and the logarithmic price are visualized.
For processing and analyzing the data in order to fulfill the weak stationarity assumptions, we transform the data into log returns according to equation \ref{eq:logreturn}.

```{=tex}
\begin{align} \label{eq:logreturn}
\mathrm{LogReturn_{t}} = \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1})
\end{align}
```

```{r log_ret, echo=FALSE, message=FALSE, fig.cap="Logarithmic returns of BTC/USD. Data are available from fall 2014 to spring 2021. The volatility clusters typical for time series are apparent. In these phases, the log returns fluctuate strongly.", out.width='85%', fig.width = 10, fig.height = 5}
# load("data/log_ret_27_03_21.rda");log_ret=    log_ret_27_03_21   # loading logreturns
# df_sub <- data.frame(date = ymd(time(log_ret)), value = as.numeric(log_ret))
# plot(df_sub,
#      type="l",
#      frame.plot = FALSE,
#      xaxt="n",
#      xlab="",
#      ylab="log return", ylim=c(-0.45, 0.2))
# axis.Date(1, at=seq(min(df_sub$date), max(df_sub$date), by="months"), format="%m-%y")
load("data/log_ret_27_03_21.rda")
log_ret <- log_ret_27_03_21
plot(log_ret, ylab="Log Return", main="Logarithmic Return BTC/USD")
```

Figure \ref{fig:log_ret} displays the historical log returns. In addition to the volatility clusters typical for financial time series, large outliers are visible. The negative outlier at the beginning of 2020 is particularly noticeable. By computing the autocorrelation (ACF) of the series in figure \ref{fig:acf_log_ret}, we can describe the dependency in these clusters. According to the ACF, the lags 6 and 10 are significant on a 5% level.

We are curious about how the log returns are distributed. Therefore, we fit a normal distribution and a Student-t distribution to the histogram of the log returns, which can be seen in figure \ref{fig:histogramm_logreturns}. Interestingly the mean is shifted slightly (0.002) to the positive side. By inspecting the tails, one can observe that the negative tail is not fitted as well as the positive part by the Student-t distribution. The two normal distributions either over- or underestimate the values in the tails, therefore we conclude that the proposed Student-t distribution fits the data better but also not perfect. Concerning the extreme outlier discussed earlier, visible in figure \ref{fig:log_ret} towards the end, the density plot makes clear how unimaginably small the probability of this extreme observation is. Although the histogram might be useful for value-at-risk considerations, for trading purposes its use is mitigated due to its complete loss of the dependency structure by plotting the returns in a density distribution.

\newpage

```{r acf_log_ret, echo=FALSE, message=FALSE, fig.cap="Autocorrelation function of BTC log returns during the entire time period from fall 2014 to spring 2021. Lag 6 and 10 are significant (values exceed the blue dotted line) and thus have an impact on possible models.", out.width='70%', fig.width = 9, fig.height = 5}
chart.ACF.phil(log_ret, main="Autocorrelation of log returns", maxlag = 20, ymax = 0.07)
```

&nbsp;

&nbsp;

&nbsp;


```{r histogramm_logreturns, out.width='70%', fig.cap='This histogram of BTC log returns illustrates how well it fits to different distributions. The blue t-Student distribution would fit the data better in the tails, while the yellow normal distribution would disperse a little better around the expected value. The distribution should be taken with a grain of salt, as the time dependence of the volatility is not taken into account.', echo=FALSE,fig.align="center", fig.width = 9, fig.height = 5}
# knitr::include_graphics("images/histogram_logret_3.jpeg")

load("data/log_ret_27_03_21.rda")  # loading logreturns closing! data xts
logret=log_ret_27_03_21 

my_data=as.numeric(logret[,1])   # as vector 
fit <- fitdistr(my_data, densfun="normal")  
fit2<- fitdistr(my_data, "t", start = list(m=mean(my_data),s=sd(my_data), df=1), lower=c(-1, 0.001,1)) #algo braucht startwerte
dt_ls <- function(x,mu,sigma,df){1/sigma * dt((x - mu)/sigma, df)}   # converting scale and location in only df

# plotting the data
hist(my_data, pch=20, breaks=70, prob=TRUE, main="Logreturns BTC",xlab="logrets")
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col="red", lwd=1.5, add=T)
curve(dt_ls(x,fit2$estimate[1], fit2$estimate[2],fit2$estimate[3]), col="blue", lwd=1.5, add=T)
curve(dnorm(x, fit$estimate[1], 0.025), col="orange", lwd=1.5, add=T)
legend("topleft", inset=.02, legend=c(TeX(sprintf("$N(\\mu = 0.00201$,$ \\sigma = 0.03925)$")), TeX(sprintf("$Student-t(l = 0.00245$,$ s = 2.0341$,$ df = 2.0341)$")), TeX(sprintf("$N(\\mu = 0.00201$,$ \\sigma = 0.025)$"))),
       col=c("red", "blue", "orange"), lty = c(1,1,1), cex=1, horiz=F, bty = "n")
```


\newpage

### 3.2. Network Architecture {#networkarchitecture}

As mentioned in section [2.1.3](#MLP), choosing an appropriate network architecture for Bitcoin price prediction is a crucial step in order to achieve useful forecasts while avoiding overfitting. Due to the complexity as well as the non-linearity of neural networks, the interpretation cannot be performed intuitively. For this reason, an approach is pursued in which neural networks with different numbers of layers and neurons are compared with each other by using the MSE loss and Sharpe ratio. This allows us to compare accuracy, respectively trading performance and possibly see a connection with network architecture.

To find the optimal network architecture, we test a maximum of 3 layers with a maximum of 10 neurons each. More complex models are not included in this thesis, as this would exceed the time frame. Furthermore, the application of complex network architectures for financial time series can be expected to lead to overfitting and thus to no real added value. The simplest network has one layer with one neuron (1), while the most complex has 3 layers with 10 neurons each (10,10,10). The total number of different combinations can be expressed as follows:

```{=tex}
\begin{align} \label{eq:comb}
\text{Comb}=\sum_{i=1}^{L}N^{i}
\end{align}
```
with:

$L=\text{maximum Layer} \in \mathbb{N}^{*}$

$N=\text{maximum Neuron}\in \mathbb{N}^{*}$

$\text{Comb} =\text{Number of all combinations}$

Thus, with our initial setup, we obtain a maximum neuron-layer combination of 1,110. To respond to the challenges mentioned in section [2.1.4](#challenges), not only a single network per neuron-layer combination is trained, but a whole batch of 50 networks. So, we end up with a total of 55,500 trained networks. For each network, the in-sample and out-of-sample MSE as well as the Sharpe ratios are determined. We use these values to find an optimal network architecture based on the statistical error as well as on the trading performance (daily signum trading).

To ensure that the neural network with the chosen network architecture does not produce trustworthy predictions only for a specific time period, the in-sample and out-of-sample splits are examined for different time periods.

\newpage

#### 3.2.1. Defining Train and Test Samples {#test_train}
&nbsp;

We are looking for an optimal network, the optimal network should also provide reasonable and reliable predictions for different periods. For further analysis, we use a subset of the introduced closing prices of Bitcoin. Starting from the first of January 2020 to the 27th of March in 2021, we only consider 15 months for our data.



We do not believe that data older than one year provides useful information for predicting the following day. The reason is that the market environmnet is constantly changing and institutional investors are not yet in agreement about Bitcoin as an asset class. By optimizing our models we found that more data would offer no additional performance, therefore the selected subset should be sufficient. Regarding consistency, the terms train and test set are used in the same sense as in-sample and out-of-sample. As proposed in [@nn_trading] we choose a test/train split from 6 months in-sample and 1 month out-of-sample. This split is applied to the whole subset in form of a rolling window. By stepping forward with this 6/1 split by step length of one month we end up with 9 data splits in total. In figure \ref{fig:test_train} this procedure is visualized, for every new timestep, a new month is considered for the out-of-sample and the first month of the in-sample falls out of the frame. 


```{r test_train, out.width='85%', fig.cap='Nine different in- and out-of-sample splits. The 6-month training phase (in-sample) is indicated in blue and the 1-month test phase (out-of-sample) in green. The splits capture different phases of the time series. Both very turbulent phases (split 1) and rather calm phases (split 5) can be recorded.', echo=FALSE, fig.width = 10, fig.height = 6}
# knitr::include_graphics("images/test_train_split_pp.jpeg.jpg")
load("data/dates_mat.rda")

subseti <- logret["2020-01-01::2021-03-27"]

# r graph gallery
# Create a df
df_sub <- data.frame(date = ymd(time(subseti)), value = as.numeric(subseti))

plot(df_sub,
     type="l",
     lwd=1.5,
     frame.plot = FALSE,
     xaxt="n",
     xlab="Time:  Month-Year",
     ylab="Log Return BTC",
     ylim=c(-0.2, 0.2),
     main= "Train/ Test Split")
axis.Date(1, at=seq(min(df_sub$date), max(df_sub$date), by="months"), format="%m-%y")



# Transform timestamp to numeric for the plot
ybot <- par('usr')[3]
ytop <- par('usr')[4]

ydist <- abs(ybot - ytop)
ypartial <- ydist/9

for (i in 1:9) {
  rect_borders <- as.numeric(ymd(dates_mat[i,]))
  
  rect(xleft = rect_borders[1],
       xright = rect_borders[2]+1,
       ybottom = par('usr')[3]+(ypartial*(i-1)),
       ytop = par('usr')[3]+(ypartial*i),
       col="#0000FF1A")
  rect(xleft = rect_borders[3],
       xright = rect_borders[4]+1,
       ybottom = par('usr')[3]+(ypartial*(i-1)),
       ytop = par('usr')[3]+(ypartial*i),
       col="#80FF001A")
}

text1 <- as.numeric(ymd(dates_mat[1,]))
posx <- (text1[4] + text1[1])/2
posy <- par('usr')[3] + ypartial/2
text(x=posx, y=posy, labels="1", cex = 1.1)
text(x=text1[4]+55, y=posy, labels="t+1 month", cex = 0.8)
arrows(x0=text1[3]+15, y0=posy, x1=text1[3]+50, y1=posy, length = 0.12, angle = 20)

text1 <- as.numeric(ymd(dates_mat[2,]))
posx <- (text1[4] + text1[1])/2
posy <- par('usr')[3] + ypartial/2 + ypartial
text(x=posx, y=posy, labels="2", cex = 1.1)
text(x=text1[4]+55, y=posy, labels="t+1 month", cex = 0.8)
arrows(x0=text1[3]+15, y0=posy, x1=text1[3]+50, y1=posy, length = 0.12, angle = 20)

text1 <- as.numeric(ymd(dates_mat[3,]))
posx <- (text1[4] + text1[1])/2
posy <- par('usr')[3] + ypartial/2 + ypartial*2
text(x=posx, y=posy, labels="...", cex = 1.1)

text1 <- as.numeric(ymd(dates_mat[8,]))
posx <- (text1[4] + text1[1])/2
posy <- par('usr')[3] + ypartial/2 + ypartial*7
text(x=posx, y=posy, labels="...", cex = 1.1)

text1 <- as.numeric(ymd(dates_mat[9,]))
posx <- (text1[4] + text1[1])/2
posy <- par('usr')[3] + ypartial/2 + ypartial*8
text(x=posx, y=posy, labels="9", cex = 1.1)


legend("topleft", inset=.02, legend=c("In-sample 6 months","Out-of-sample 1 month"),
       col =c("#CCCCFF", "#CCFFCC"),  pch=c(15,15),cex=1,2, horiz=F,pt.cex=2,
       bty = "n")
```

In the time series in figure \ref{fig:test_train}, one can see different periods. Strongly volatile as well as rather calm phases occur. With the rolling window, we can train and test the networks based on different phases. Thus, we can also evaluate the performance of the networks based on different phases and not only on a predefined single test and train split.

The complexity of the search for the optimal network architecture increases significantly here. With the conditions defined for us, we train and test a total number of 499,500 networks to define the optimal network.

\newpage

#### 3.2.2. Defining Input-Layer {#input-layer}
&nbsp;

To train the feedforward neural networks, we need to specify the input layer in addition to the network architecture. For this, we take the autocorrelation function of the time period specified earlier.

In contrast to the ACF's of the entire data set shown in figure \ref{fig:acf_log_ret} where lags 6 and 10 have a high impact, here in figure \ref{fig:test_train_acf} we see that lags 4 and 10 are significant. If one uses the ACF's as an indicator for the specification of a model, it must be considered that for other time periods also other rules apply. For modeling the entire data set, one could safely specify the input layer up to a lag of 10. Here for our subset, we specify the input layer up to a lag of 7. Lag 7 is just significant and could potentially explain an important dependency in the data. Since Bitcoin can be traded 7 days a week, including weekends, the dependency of 7 lags in the ACFs would also be explained.

&nbsp;

```{r test_train_acf, out.width='85%', fig.cap="Autocorrelation function of our chosen time period. One can clearly see an increased influence of lags 4 and 7, not as in the ACF's of the entire data set with significant values at 6 and 10. Only late at lag 18 does the ACF show a significant value again.", echo=FALSE, fig.width = 9, fig.height = 5}

chart.ACF.phil(subseti, maxlag = 20, ymax = 0.12, main="ACF of time period: 2020-01-01/2021-03-27")
```

For the following procedure for finding an optimal network architecture we use an input layer of 7 lags. Since we will also use the same time period later in the trading section [3.3](#trading-strat), we will also specify the input layer with 7 lags for training these networks.

\newpage

#### 3.2.3. Neural Network Training {#train-nn}
&nbsp;

For training the networks, we use the function `neuralnet()`, which is part of the R-package `neuralnet`. The advantage of this function is the utilization of the backpropagation algorithm presented in section [2.1.2](#backprogation_algorithm). Various parameters must be passed to the function. We pass the 6-month in-sample data sets defined in section [3.2.1](#test_train), which are specially tailored for our applications. In the previous section, we decided on a maximum delay of 7 and the structure of the input layer can be seen in table \ref{tab:data_tab}. The pink-colored values indicate that they are the same values but shifted. These are the input data of the 5th test period, which would start in 2020-05-01. However, since we shift the data by 7-time units, we also lose 7 days of data, since no values are known before 2020-05-01 to fill the delays with data (of course values exist before, but so the procedure can be applied to all possible time periods). If, for example, we considered a larger number of delays, then the data set would accordingly also become smaller and smaller.

```{r data_tab, echo=FALSE}
load("data/data_obj_5.rda")
name <- c("lag0", "lag1", "lag2", "lag3", "lag4", "lag5",
          "lag6", "lag7")  

subseti <- as.data.frame(round(head(data_obj_5$data_mat, 5), 5))
colnames(subseti) <- name
kbl(subseti, caption="Adapted data of the 5th test/train split for training neural networks with the R package neuralnet. Lag0 corresponds to the original data while the rest are simply the corresponding lag of the original data. The color-coded values show that these are simply shifted values.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  column_spec(2, color = c("#E413A3", rep("#000000", 4))) %>%
  column_spec(3, color = c("#000000", "#E413A3", rep("#000000", 3))) %>%
  column_spec(4, color = c(rep("#000000", 2), "#E413A3", rep("#000000", 2))) %>%
  column_spec(5, color = c(rep("#000000", 3), "#E413A3", "#000000")) %>%
  column_spec(6, color = c(rep("#000000", 4), "#E413A3"))
```

&nbsp;

In addition to the training data, we pass the function an equation to specify how the training data should be handled. In our application, we aim to find a reliable prediction for the original value from the lagged data and define the equation for the function as follows:

$$lag0 \sim lag1+lag2+lag3+lag4+lag5+lag6+lag7$$

The function thus uses the delayed data 1 to 7 as input layer and trains a network to get as close as possible to the true values of lag0. As discussed in section [3.2](#networkarchitecture), we would like to investigate the impact of network architecture on network performance. Thus, for each split, we pass the corresponding architecture from (1) to (10,10,10) to the function.

In the `neuralnet`-function, we use the sigmoid logistic function as the activation function. As mentioned in section [2.1.2](#backprogation_algorithm), the logistic function returns values between 0 and 1. According to the article by S. Heinz, neural networks benefit when the input layers are scaled according to the activation function [@heinz]. Solving the numerical optimization problem is therefore simplified. If you look at the logistic function in figure \ref{fig:sigmoid}, the activity of neurons close to zero is higher than at extreme values. Extreme positive or negative values would result in 0 or 1 according to the activation function and thus no longer make a large contribution to the model. The numerical problem would be much more complex to solve than having values within an active band. Thus, we scale the data to a range of [0, 1] that is convenient for numerical optimization. The lowest value in the data set is scaled to 0, while the highest value is scaled to 1. The listed values from table \ref{tab:data_tab} are transformed and can be seen in table \ref{tab:data_tab_scaled}.

\newpage

```{r data_tab_scaled, echo=FALSE}
name <- c("lag0", "lag1", "lag2", "lag3", "lag4", "lag5",
          "lag6", "lag7")  
subseti <- as.data.frame(round(head(data_obj_5$train_set, 5), 5))
colnames(subseti) <- name
kbl(subseti, caption="Scaled data from the 5th test/train split. This is the same data as in the previous table, only here the data is scaled between 0 and 1.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  column_spec(2, color = c("#E413A3", rep("#000000", 4))) %>%
  column_spec(3, color = c("#000000", "#E413A3", rep("#000000", 3))) %>%
  column_spec(4, color = c(rep("#000000", 2), "#E413A3", rep("#000000", 2))) %>%
  column_spec(5, color = c(rep("#000000", 3), "#E413A3", "#000000")) %>%
  column_spec(6, color = c(rep("#000000", 4), "#E413A3"))
```

&nbsp;

The result from the `neuralnet`-function is the prediction of the network for the in-sample test data set, i.e. the data with the lag0. To compare the performance in the in-sample area with other networks, the results computed from the scaled data by the network are transformed back to the original form.

For the out-of-sample area, the `predict` function from the same package is used. One passes the trained network (optimized weights and biases) with the scaled out-of-sample data (lag 1 to 7). This result is also transformed back and compared to the original data (lag0) to analyze the performance of the neural network, ouf-of-sample.

\newpage

#### 3.2.4.  Evaluating Network Architecture {#evaluate_nn}
&nbsp;

Here we would like to focus on some findings that we discovered during the processing of the trained networks. To illustrate the results, an extract is discussed here, namely only the 5th train/test split (in figure \ref{fig:test_train} the middle one).

The plot in figure \ref{fig:mse_plot1} compares one layer networks with different numbers of neurons with each other. Networks with a maximum of ten neurons are compared. These different configurations can be seen on the x-axis. The first data point corresponds to a simple network with one neuron. The y-axis shows the MSE values obtained with the respective trained model. As already explained, we use 50 different optimizations of each configuration to get a better idea of a potentially systematic relationship with the MSE. In the plot, each of the 50 configurations is drawn using a different color.

```{r mse_plot1, out.width='100%', fig.cap="In- and out-of-sample MSE of the 5th time split. Only the networks with only one layer are shown here. Simplest network has 1 neuron (leftmost) and the most complex one has 10 neurons (rightmost), therefore we have the MSE's of 10 different neural networks. The in-sample MSE's tend to decrease, while the out-of-sample MSE's move the opposite direction.", echo=FALSE, fig.align="center"}
knitr::include_graphics("images/batch5/mse_5_layer1.jpeg")
```

What is already noticeable here is that with an increasing complexity, i.e. with increasing number of neurons, the in-sample MSE decreases. The in-sample forecasts are thus becoming more accurate. At the same time, you can see how the out-of-sample MSE increases with increasing complexity, which means that the forecast accuracy tends to get worse.

\newpage

If you add another layer to the network architecture, the number of different networks with the same number of layers also increases. In the following figure \ref{fig:mse_plot2}, the simplest network is a (1,1) network. So 2 layers with one neuron each. The most complex is a network with a (10,10) architecture.

As noted earlier in figure \ref{fig:mse_plot1}, the values for the MSE also fluctuate more and more with increasing complexity. Small in-sample MSE for more complex networks leads to rather high out-of-sample MSE. This leads us to the previously mentioned challenges in section [2.1.4.1](#overfitting), and that too many estimated parameters can lead to overfitting of the network.

```{r mse_plot2, out.width='100%', fig.cap="In- and out-of-sample MSE of the 5th split. Shown here are only the networks with two layers. The simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 100 different neural networks. As in the previous graph, in-sample MSE's tend to decrease while out-of-sample MSE's tend to increase.", echo=FALSE,fig.align="center"}
knitr::include_graphics("images/batch5/mse_5_layer2.jpeg")
```

Looking at the out-of-sample MSE's in the graph below in figure \ref{fig:mse_plot2}, we can see lines that are outside of the blue rectangle. These values are extreme outliers that indicate the randomness of neural networks. This again confirms that choosing an optimal network over several equal networks (50 in our case) makes more sense than making the choice depend on only one randomly trained network. Depending on which solution the training algorithm finds, the results can be very different. The y-axis was scaled for better comparability of in-sample and out-of-sample, but one loses the overview of how much the outliers differ from the rest.

\newpage

Lastly, we look at the results of the different network architectures with a third layer. In figure \ref{fig:mse_plot3}, we can see very well the inverse correlation between the in-sample and out-of-sample MSE. Again, the in-sample MSE gets better with increasing complexity while the out-of-sample MSE gets worse. There is also a certain recurring pattern that is striking. After a certain complexity, the in-sample MSE decreases steadily and then increases abruptly. The opposite pattern can also be observed out-of-sample. These patterns emerge during transitions from more complex to more simple architectures. For example, the transition from a model with (8,10,10), with a total of 28 neurons, to a model with (9,1,1) with only 11 neurons.

It is interesting that at the beginning, with the rather simple model architectures, the MSE of all realizations is very constant and only varies very slightly.

```{r mse_plot3, out.width='100%', fig.cap="In- and out-of-sample MSE of the 5th split. Shown here are only the networks with 3 layers. Simplest network has an architecture of 1 neuron per layer (leftmost) and the most complex has 10 neurons per layer (rightmost), therefore we have the MSE's of 1000 different neural networks. Same pattern as for 1 layer and 2 layers: The in-sample the MSE's decreases, while the out-of-sample MSE's increase.", echo=FALSE,fig.align="center"}
knitr::include_graphics("images/batch5/mse_5_layer3.jpeg")
```
\newpage

Figure \ref{fig:mse_plot5} shows the MSE of the models with 1-3 layers, i.e. the last three plots side by side. As mentioned, it can be seen here that the in-sample MSE scatter more towards the bottom as the complexity of the architecture increases. This does not have a positive effect on the out-of-sample, since in the same area the MSE deteriorates massively (note the different scaling of the y-axes). As a result, we have no real added value from more complex models. Also, to be noted is that the in-sample MSE does not get worse than a certain threshold at the upper boundary. This asymmetric scattering around this value is likely due to the numerical characteristics of the optimization algorithm.  

At this point, it should be emphasized that only the analyses from time split 5 are visualized in this section. Our primary goal is to compare the performances of different network architectures using MSE to find the optimal network that performs well for every time split. However, finding an optimal architecture using such visual analysis of the MSE seems nearly impossible. Nevertheless, the main finding of this section is that the MSE deteriorates massively with more complex models and thus a simpler one should be considered. Equally remarkable is the fact that the same model architectures produce such different results. Whilst many models range in a more or less solid midfield, traits of overfitting can be recognized. These are reflected by the spikes in the out-of-sample MSE.

```{r mse_plot5, out.width='100%', fig.cap="In- and out-of-sample MSE of the 5th split. The 3 previous graphs are summarized here and represents an overview of all 3 layers, which results in a total of 1110 neuron-layer combinations. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.", echo=FALSE,fig.align="center"}
knitr::include_graphics("images/batch5/mse_5_all.jpeg")
```

What has not yet been studied is the dependence of the neural network's behavior on the different window splits we defined in figure \ref{fig:test_train}. Considering that the same network architectures provide MSE's of different quality (including huge outliers), the results for each configuration are summarized using a robust method. We make use of the median of the MSE's of all 50 equal networks across all time splits in order to evaluate the accuracy of the corresponding model. We consider this a better method than the arithmetic mean as figure \ref{fig:mse_plot5} shows large outliers. Thus, it can be better investigated whether the corresponding network architecture provides sound results apart from this one outlier. 

\newpage

The medians of the MSE's of all 50 equal networks across all time splits are plotted in figure \ref{fig:mse_median}. We restrict ourselves to neural networks with 1-2 layers (recognizable in the red (1) and blue (2) rectangles), since it can be assumed that too complex models are not suitable for the target. The lines represent the medians of the MSE of all 50 optimized neural networks at a given network architecture (x-axis). The nine different colors specify the specific time split in which the neural networks have been trained and tested. We anticipate that this comparison will facilitate finding a network architecture that performs across all time splits. 

```{r mse_median, out.width='100%', fig.cap="We take the median of all MSE's for a given network architecture. The 9 splits are visualized by different colors. For each network architecture, 50 neural networks were trained (55`500 models per time split). The nine colors illustrate how these models behave in the different time splits. Time periods with a strong trend (for example, the dark red line, split 7) lead to the same structure as already noted in the previous graphs. Again, the inverse relationship is evident. The more complex a network, the smaller the in-sample MSE's become. At the same time, the out-of-sample MSE's increase.", echo=FALSE,fig.align="center"}
knitr::include_graphics("images/batch5/median_mse.jpeg")
```

First, there is evidence of overfitting in the medians as well. In particular, splits 1-6 show a somewhat expected picture: the in-sample error becomes smaller with more complex architecture, while the opposite can be seen in the out-of-sample. However, the periodically appearing spikes visible in the time splits 8 and 9 of the in-sample plot seem unnatural. The plausible difference between this and the other splits is the underlying data. Therefore, we take a look at what the initial prices of Bitcoin are doing in this period. The plots with the time splits of the logarithmic prices can be found in the appendix starting with figure \ref{fig:price1}. As a rule of thumb, the neural network behaves better when the train and test pairs behave similarly. In the plots of splits 8 and 9, it is clearly visible that in each case the in-sample shows a bullish behavior. This trend is not continued in the out-of-sample part, which probably leads to biased predictions due to amplified dependence structures.

\newpage

Our goal in the second part of this thesis is to work out a trading strategy with a suitable neural network. Therefore, as the last comparison, we visualize in figure \ref{fig:sharpe_median} how well the different network architectures behave in sign trading. This is simple trading which depends on the sign of the prediction $\hat{y}_{t+1}$ i.e. next expected log return. If a positive prediction is forecast, the trader is in a long position, otherwise in a short position. As with the previous plot, the Sharpe ratios of each neural network realization perform differently despite having the same network architecture. Therefore, we again decided to plot the median of all Sharpe ratios with the same network architecture. The nine different colors indicate the time interval during which the neural networks were trained and tested.

```{r sharpe_median, out.width='100%', fig.cap='We take the median of all Sharpe ratios for a given network architecture. The 9 splits are visualized by different colors. The in-sample Sharpe ratios increase (upper plot) with increasing complexity. The out-of-sample Sharpe ratios tend to decrease with increasing complexity. In comparison to the previous plots using the MSE, the effect is much smaller here.', echo=FALSE,fig.align="center"}
knitr::include_graphics("images/batch5/median_sharpe.jpeg")
```

When looking at the Sharpe ratios, an inverse relationship between the in-sample and out-of-sample can be seen. The in-sample Sharpe ratio improves in most time splits with increasing complexity. In the out-of-sample, however, the medians decrease. It can also be seen that some of the Sharpe ratios show periodic spikes again. Apart from these aspects, no clear patterns or correlations could be identified. To put these numbers into perspective, a one-time investment in the S&P 500 ten years ago would have resulted in a Sharpe ratio of 0.83 [@S&P500_sharpe].

\newpage

#### 3.2.5. Model Selection {#mod-select}
&nbsp;

With the above procedure, we have been able to identify the characteristics of neural networks. In time series applications, complex networks lead to overfitting. We could detect an inverse relationship between in- and out-of-sample MSE. We trained many networks, yet no distinct optimal neuron-layer combination can be identified. Determining the architecture based on the minimum MSE would not yield good results. There would be too great a risk that the chosen architecture would be too complex and lead to overfitting. Additionally, the chosen architecture should lead to trustworthy predictions over different time periods (in our example over the 9 splits).

For the rest of the procedure, we decide to use a neural network architecture of (7,7). According to the 2-layer structure visualized in figure \ref{fig:mse_plot2}, the architecture we chose looks very balanced. In-sample, the MSE is rather in a more optimal range (smaller MSE) and out-of-sample, any overfitting is rather minimal compared to even more complex networks. In the attachment in figure \ref{fig:meanmean2}, you can see the average MSE across all 9 splits. The neuron layer architecture of (7,7) is exactly in the range where the in-sample MSE decreases and out-of-sample increases. The selected network lies exactly between the volatile networks, thus this architecture does not seem to be as prone to overfitting as the one next to it. The (7,7) network is not too complex, tends to have a good in-sample MSE and the out-of-sample MSE does not tend to overfit as much.

#### 3.2.6. Benchmark {#benchmark}
&nbsp;

The trained networks and applied methods are compared with each other using a benchmark. For this purpose, we use the buy-and-hold, which is a long-term passive investment strategy. Investing with this strategy often leads to better returns in the long run than active trading [@bnh]. This strategy is hard to beat, especially when the market is trending upwards. In such a trend, an active strategy would hardly recommend anything other than buy-and-hold.

We, therefore, compare our developed active strategies with the cumulative log returns. Our benchmark includes all green out-of-sample returns from figure \ref{fig:test_train} cumulated.

\newpage

### 3.3. Trading Strategy {#trading-strat}

This section describes the trading strategy in more detail. In basic terms, the findings of the previous sections are extended with considerations from explainable artificial intelligence (XAI) as well as from more traditional tools used in time series analysis. The following flowchart in figure \ref{fig:flowchart_trading} shows a broad overview of how the different factors are combined to generate the final trading signal.

&nbsp;

```{r flowchart_trading, fig.align='center', out.width='100%', fig.cap='This flowchart illustrates an overview of the trading strategy applied in this section. After finding the best network architecture for accurate forecasts of log returns, we check the stability of the neural network with XAI. The GARCH-model addresses the phenomenon of volatility clusters. Finally, all these information are combined in a trading strategy in order to be compared with our benchmark consisting of buy-and-hold.', echo=FALSE}
knitr::include_graphics("images/flowchart_trading.png")
```

&nbsp;

The main component is the prediction of the neural network. This reflects the expected log return of the next day and is thus an important indicator of whether the money should remain invested or not. However, the output of these neural networks are to be treated with caution, as we have seen in the section [3.2.4](#evaluate_nn). Therefore, we rely on XAI to detect instability and incorporate this information into the final trading signal. Furthermore, volatility persistence is observed in financial time series, i.e. large changes can be expected after large changes. Leverage effects are also probable, which means that the tendency to achieve a negative return is higher when volatility is large. A GARCH model is used to model these phenomena, whose volatility predictions provide important information for the final trading signal.

While these methods sound promising, it should not be forgotten that a buy-and-hold strategy (given nerves of steel) also led to remarkable performances in the past that even outperformed traditional asset classes. The price development is described in sections [2.4.1](#historical_analysis) and [2.4.3](#bitcoin_valuation). That is why buy-and-hold is frequently used as a comparison. 

Before we trade the returns, we first need to define the environment and make some assumptions. Normally, trading costs are crucial in high-frequency trading. Since the fee structure for cryptocurrencies is more attractive than for stocks, transaction costs are waived for the sake of simplicity.  Further, we assume the possibility of entering short positions for BTC. These assumptions allow us to either stay in the market (signal = 1), to exit the market (signal = 0) or to sell tomorrow's return (signal = -1). In the latter two positions, negative returns can be avoided, or we can even profit from the downward movement. By exiting the market with signal 0, there are several opportunities to invest the money elsewhere or just stay out of the market and therefore eliminate market exposure. 

\newpage

#### 3.3.1. Trading with Neural Networks and LPD {#trading_lpd_nn}
&nbsp;

The following consecutive sections [3.3.1.1](#network) and[ 3.3.1.2](#lpd-signal) go together. We describe, how in this thesis the neural network and LPD( theory explained in section [2.2.2](#xai_finance)) is used to create  trading signals. 



##### 3.3.1.1. Neural Network {#network}
&nbsp;

In [3.2.](#networkarchitecture) we use an average of only 50 realizations of feedforward nets to reduce computation time. However, it is proposed by the Central Limit Theorem [@central_limit_theorem] to use an approximate $\text{number of iterations } N_{total} \ge 100$ for an accurate mean value. 

For a better understanding, we will take a closer look at an example of the effects of averaging all predictions. Assume we calculate an amount of $N_{total}$=100 nets in total and try to predict just 1 day and use the sign of the forecast as a trading signal. If 49 nets predict a positive forecast and 51 a negative, we end up with a negative trading signal, which is actually decided by just one net. This means that the resulting trading signale occurs randomly. A positive trading signal that results from 60 positive predictions and 40 negative ones may be more accurate. Therefore, the following a function is introduced that takes this aspect of majority voting into account.

&nbsp;

Let $\hat{\underline{p}}$ be the predicted values {$\hat{p}_{1}\dotsc\hat{p}_{N_{total}}$} of the neural networks in a certain time $t$.
With function $f$ \ref{eq:net_decision1} in every $t$ a trading signal is derived by majority decision. How clearly the decision of the forecasts must be, is decided by parameter $\kappa$.
For further evaluation of neural networks $\kappa$ can be used as an optimizing factor.

&nbsp;
 
```{=tex}
\begin{equation}\label{eq:net_decision1}
 f(\hat{\underline{p}}) =
\begin{cases} 
  0      & \quad \text{if } \frac{1}{N_{total}}  \sum_{i=1}^{N_{total}}sign(\hat{p}_{i}) < \kappa \\
  sign(\sum_{i=1}^{N_{total}}sign(\hat{p}_{i}))    & \quad \text{else}
  

   \end{cases}
\end{equation}
```

&nbsp;
  
$N_{total}$ = Total number of neural networks

$\kappa$ = Ratio of majority decision 

$\hat{p}_{m}$ = Predicted value from neural network, $m \in$ {$1 \dotsc N_{total}$}

&nbsp;

One can observe in function $f$ how the uncertain decisions result in a 0 signal. You could also use position sizing and adjust the signal depending on the size of $\kappa$. But we have omitted this aspect, because otherwise there are too many parameters in the model.

\newpage

##### 3.3.1.2. LPD Signal {#lpd-signal}
&nbsp;

To explain the following as understandable as possible, we recall chapter [2.2.2](#xai_finance) where the discrete derivatives of each input layer $lag_{j}$ were calculated with respect to the forecast at each time step. This procedure is now applied to the in-sample as well as the out-of-sample area. From the in-sample, the standard deviation and the mean are calculated for each $lag_{j}$. These are now used for the out-of-sample area. An example with out-of-sample 2021-01-01 - 2021-02-08 clarifies these procedure:
Figure \ref{fig:lpd_explain} represents the 7 lags, when looking at lag 4, the top line shows the previously calculated mean value from the in-sample (black) and the in-sample standard deviation upwards and downwards (dashed red). For the visualization the lines are drawn only for one lag.
Let us take a look at the time step 2021-01-25, where you can see that the pink line exceeds the standard deviation.  Now we count in this time step how many other lags also overshoot their standard deviation.
This is shown in formula \ref{eq:net_decision2}, we see immediately that on 2021-01-25, 4 lags have exceeded their standard deviation by the mean value.
In the example we state that if 4 or more lags exceed the upper limit the final signal is 0 and if between 2 and 4 lags exceed the standard deviation, we assign the signal 0.5. For our example date 2021-01-25 the signal would be 0.
Correctly concluded, we get a trading signal for each date.

&nbsp;

```{r lpd_explain, fig.align='center', out.width='60%', fig.cap="The LPD is used to generate the trading signal. Higher volatility indicates that the neural network may provide unreliable outputs. For the top LPD time series, the mean value and the upper and lower decision bands are plotted. Each time this LPD exceeds the upper or lower decision band, a signal is realized for this lag. In the most extreme phases it can happen that several lags exceed the decision bands and thus generate several signals. The number of all exceedances will be considered in the next plot in order to generate the XAI trading signal.", echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}
knitr::include_graphics("images/xailpd/out_decision_small.jpeg")
load("data/xai/7_7_withsignal_xai_in/OLPD_mat_out_plot.rda")
load("data/xai/7_7_withsignal_xai_in/meanCI.rda")
plot.xts(OLPD_mat_out["::2021-02-08."],
         col=c("#003f5c", "#665191", "#d45087", "#f6004a", "#ff1208", "#ffa600", "#ff7c43"),
         main="LPD Out-of-Sample")
lines(meanCI$meanline,lwd=2,col="black",lty=2)
lines(meanCI$meanup,lwd=2,col="red",lty=3)
lines(meanCI$meandown,lwd=2,col="red",lty=3)
addLegend("bottomleft",
          legend.names=c(TeX(sprintf("Mean value from $lag_{j}$")), TeX(sprintf("Standard deviation from $lag_{j}*\\lambda$"))),
          col=c("black","red"),
          lty=c(2,3),
          lwd=c(2,2),
          ncol=1,
          bg="white")
```

\newpage

```{r lpd_explain_sum, fig.align='center', out.width='60%', fig.cap="The black line shows how many lags exceed the threshold defined above at one point in time. The more extreme lags are counted, the more unstable the neural network is. Red is the upper decision limit and the green is the lower one. Values below the green line indicate realiability and lead to a signal = 1. Values above the red line indicate instability and lead to a signal = 0. Values between the red and the green line lead to a signal = 0.5.", echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}
# knitr::include_graphics("images/xailpd/out_sum_2.jpeg")
load("data/xai/7_7_withsignal_xai_in/sum_explana_out_plot.rda")
load("data/xai/7_7_withsignal_xai_in/sumCI.rda")
plot(sum_explana["::2021-02-08"],type=c("b"),
     main=TeX(sprintf("Sum of lags deviating $\\lambda$ *sdY from $\\bar{Y}$")))
lines(sumCI$lineup,lwd=2,col="red",lty=1)
lines(sumCI$linedown,lwd=2,col="green",lty=1)
name=c("η upper","η lower")
addLegend("topright",
          legend.names=c(TeX(sprintf("$\\eta_{upper}$")), TeX(sprintf("$\\eta_{lower}$"))),
          col=c("red","green"),
          lty=c(1,1),
          lwd=c(2,2),
          ncol=1,
          bg="white")
```

&nbsp;

To use this behavior for optimizing our model, a more formal general approach is required. The standard deviation is therefore scaled with parameter $\lambda$ and with formula \ref{eq:count} in every $lag_{j}$ it is decided whether the predicted LPD $\hat{\Delta}_{j,t}$ exceeds $\mu_{j} \pm \lambda \sigma_{j}$.  

With function $g$ \ref{eq:count} we count the lags which exceed their bands, the output of $g$ is between 0 and $lag_{n}$. In  $\text{signal}$ \ref{eq:net_decision2}, $g$ is further applied to generate a signal. With $\eta_{lower}$ (green) and $\eta_{upper}$ (red) it is decided which final output for the timestep $t$ should be assigned. In section [3.1.](#data_exploration) last section, we observed, that larger negative returns are more likely than positive, therefore the proposed signal is 0. For values between $\eta_{lower}$ and $\eta_{upper}$ we propose a signal 0.5. Function \ref{eq:net_decision2} $\text{signal}$  is now applied to every timestep $t$, therefore the final LPD signal is computed.

\newpage


```{=tex}
\begin{align} \label{eq:Ybar}
\mu_{j}=\frac{1}{n_{in}} \sum_{t=1}^{n_{in}}\Delta_{j,t}
\end{align}
```


```{=tex}
\begin{align} \label{eq:sdY}
\sigma_{j}=\sqrt{  \frac{1}{n_{in}}  \sum_{t=1}^{n_{in}}(\mu_{j}-\Delta_{j,t})^{2} }
\end{align}
```





```{=tex}
\begin{align} \label{eq:count}
g(\hat{\Delta}_{t})=\sum_{j=1}^{lag_{n}}\Big((\hat{\Delta}_{j,t}>\mu_{j} - \lambda \sigma_{j} )\vee( \hat{\Delta}_{j,t}<\mu_{j} + \lambda \sigma_{j})\Big)
\end{align}
```

```{=tex}
\begin{equation}\label{eq:net_decision2}
\text{signal}(\hat{\Delta}_{t}) =
\begin{cases} 
  0.5    & \quad \text{if } ,  \eta_{lower} < g(\hat{\Delta}_{t}) \le \eta_{upper}    \\                                              
  0      & \quad \text{if } ,   \eta_{upper} < g(\hat{\Delta}_{t}) \\
  1      & \quad \text{else}
   \end{cases}
\end{equation}
```

&nbsp;

$\Delta_{j,t}$ = Matrix of in-sample LPD {$\underline{\Delta}_{1}...\underline{\Delta}_{n_{in}}$} observed for every t in insample

$\hat{\Delta}_{j,t}$ = Matrix of out-of-sample LPD {$\underline{\hat{\Delta}}_{1}...\underline{\hat{\Delta}}_{n_{out}}$} predicted for every t in out of sample

$\lambda$ = Scaling parameter for standard deviation

$lag_{n}$ = Maximum number of lags

$\eta_{lower},\eta_{upper}$ = Lower  and upper border for signal decision

$n_{in}, n_{out}$ = Maximum time insample / out-of-sample

$g$ = Function of lags exceeding band, output $\in$ {$0...lag_{n}$}

$signal$ = Function of trading signal

$\mu_{j}$ = Arithmetic mean of insample LPD for every $lag_{j}$

$\sigma_{j}$ =  Standard deviation of insample LPD for every $lag_{j}$

\newpage

#### 3.3.2. GARCH Volatility Predictions {#garch-signal}
&nbsp;

In a further step, we examine the time series with a traditional GARCH model that allows heteroskedasticity. T. Bollerslev proposes to use this to model time-dependent variance as a function of lagged shocks and lagged conditional variances [@garch]. Based on an ARMA(1,1)-GARCH(1,1), we conduct one-step-ahead predictions using a rolling window of size 365 days and refit the model after 30 days. This results in two different trading strategies of which one is based on the signs of the forecasts and the other on predicted volatility. The predictions of future volatilities are presented in the appendix in figure \ref{fig:vola_forecasts}. 

The first trading strategy is based on one-step-ahead rolling window forecasts (here: log return) resulting from the ARMA(1,1)-GARCH(1,1). When we predict a positive value, the algorithm decides to enter, respectively remain in a long position. When predicting a negative value, we enter a short position to benefit from the anticipated market movement. 

The second strategy is solely based on volatility predictions resulting from a GARCH(1,1) and tries to take an advantage from the asymmetric volatility phenomenon by F. Black. This phenomenon describes a negative correlation between the volatility of the return and the achieved return [@leverage_effect]. Therefore, we define the following trading rule:

```{=tex}
\begin{equation}\label{eq:vola_predict}
\text{Signal}_{GARCH}(\hat{\sigma}_{t}) =
\begin{cases} 
                                              
  0      & \quad \text{if} ,  \hat{\sigma}_{t} \ge \sigma_{historical} \\
  1      & \quad \text{else}

   \end{cases}
\end{equation}
```

$\hat{\sigma}_{t}$ = predicted volatility for every ${t}$

$\sigma_{historical}$ = historical volatility

In other words, the function checks whether the predicted volatility is significantly greater than the 95% confidence interval (based on historical data). If this is the case, the trading signal is set to 0, i.e. the position is sold respectively we stay out of the market. If the expected volatility is within the normal range, we enter respectively remain in a long position. For simplicity, a threshold of 1.64 is used, which corresponds approximately to the upper 95% confidence interval for a standard normal distribution.

\newpage

```{r garch_trading, fig.align='center', out.width='80%', fig.cap='Cumulative daily returns of two different GARCH trading strategies and a simple buy-and-hold strategy. The GARCH signum strategy is based on the ARMA(1,1)-GARCH(1,1) prediction, while the GARCH volatility strategy simply checks if the threshold is exceeded. The time periods where we quit the market is clearly visible as horizontal lines.', echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}
# knitr::include_graphics("images/GARCH/garch_trading.png")

load("data/GARCH_vola_predictions/garch_perf.rda")
load("data/GARCH_vola_predictions/sharpe_perf.rda")
par(mfrow = c(1,1))
plot(cumsum(garch_perf[,1]), main = "Cumulative daily returns", lwd=1.5)
lines(cumsum(garch_perf[,2]), col = 2, lwd=1.5)
lines(cumsum(garch_perf[,3]), col = 3, lwd=1.5)
addLegend(legend.loc = "topleft", legend = c("Buy-and-Hold", "GARCH Signum Trading", "GARCH Volatility Trading"), col = c(1,2,3), lty = 1, lwd=1.5)
```

&nbsp;

Figure \ref{fig:garch_trading} illustrates how the GARCH trading strategies shown as well as a buy-and-hold strategy would have performed in a backtesting. It is easy to see that buy-and-hold outperforms the other two. The GARCH sign strategy misjudges the situation at key points in time and thus hurts the overall performance. Only during the Covid-19 crash in March 2020, the GARCH sign strategy suffered a smaller loss. On the other hand, we missed the following bull run. Also, the horizontal lines in the GARCH volatility strategy are clearly visible at which the predicted volatility is too large and thus the market exits. While some draw-downs are dampened, upside moves are also avoided. 

\newpage

#### 3.3.3. Neural Network and LPD Trading
&nbsp;

In section [3.2.5](#mod-select) we have decided to fix the architecture to 2 layers with 7 neurons each, all
the 9 splits which were introduced in section [3.2.1](#test_train), are now used for trading. In every 6 month-insample split a new model is estimated with the schema from [3.3.1](#trading_lpd_nn). The out-of-sample signals of the neural network and LPD from every split are summarized. 

Our previous findings from the GARCH model were not very promising, nevertheless, we try to use those signals with the neural net output and the signals from LPD. That leaves us with a total of three signals for every timestep: 

&nbsp;

$\hat{NN}_{t}$ = Neural net signals $\in$ {$-1,0,1$}

$\hat{LPD}_{t}$ = LPD signals $\in$ {$0,0.5,1$} 

$\hat{GARCH}_{t}$ = GARCH signals $\in$ {$0,1$} from section [3.3.2](#garch-signal)

&nbsp;

With the previously established architecture, all combinations of the signals for different values for the parameters: $\lambda$, $\eta_{lower},\eta_{upper}$ and $\kappa$  are computed and the performance is compared to each other as well as to the benchmark from [3.2.6](#benchmark). To find a balance between precision and computation time, we found $N_{total}$= 1000 Neural networks in every split, should be sufficient for the evaluation.

The most promising performance is found with $\lambda = 1$, $\eta_{lower} > \eta_{upper}$ (no 0.5 signals were used) $\eta_{upper}=3$ and $\kappa = 0.2$.
In figure \ref{fig:perf} different combinations are visualized.

- NN is simply trading the $LogReturn_{t}$ with $\hat{NN}_{t}$ 

- NN+LPD is the product of NN and $\hat{LPD}_{t}$ 

-  NN+LPD+GARCH takes into account the GARCH signals $\hat{GARCH}_{t}$ by multiplying them with NN+LPD.

By investigating the plot, it seems that some combinations are performing better than the others. The GARCH application appears to worsen the performance. The ligthblue and the yellow line catches our eye, their performance seems to be better than the rest and as we can see, those are combinations of NN+LPD. Notice that signals with zeroes are not yet invested elsewhere, therefore they represent staying out of the market, mitigate risk and wait for a new signal to come.

&nbsp;

```{r perf, fig.align='center', out.width='80%', fig.cap="Different combinations of trading signals are compared to each other. The best performing one is buy-and-hold (green). It is closely followed by the light blue NN+LPD combination with a $\\kappa$ of 0.2. The behavior of the individual rules is often very similar, few different decisions can influence the performance. Especially at the end of July, beginning of August the rules behave differently, but then usually behave very similarly.", echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}

# knitr::include_graphics("images/xailpd/lambda_1/perf.svg")

load("data/xai/7_7_withsignal_xai_in/perfall_without_eth.rda")
colors= c("red","pink","violetred","darkorchid","blue","lightblue","turquoise","dodgerblue4","darkorange","goldenrod1","yellow","darkgoldenrod1","green")
plot.xts(compare_perf,
         main=TeX(sprintf("Performance cumulated from 9 Splits, $\\lambda = %d$", 1)),
         col=colors)
addLegend("topleft",
          legend.names=c(TeX(sprintf("NN $\\kappa = %.1f$", 0.1)),
                         TeX(sprintf("LPD $\\kappa = %.1f$", 0.1)),
                         TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.1)),
                         TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.1)),
                         
                         TeX(sprintf("NN $\\kappa = %.1f$", 0.2)),
                         TeX(sprintf("LPD $\\kappa = %.1f$", 0.2)),
                         TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.2)),
                         TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.2)),
                         
                         TeX(sprintf("NN $\\kappa = %.1f$", 0.3)),
                         TeX(sprintf("LPD $\\kappa = %.1f$", 0.3)),
                         TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.3)),
                         TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.3))
                         ,
                         TeX(sprintf("Buy and Hold"))),
          col=colors,
          lty=c(rep(1,13),2),
          lwd=c(rep(2,13),3),
          ncol=1,
          bg="white")
```

\newpage

When we have a closer look at the Sharpe ratio in \ref{fig:sharpe}, our findings from before are confirmed. The NN+LPD with $\kappa = 0.2$ has even a better Sharpe ratio than buy and hold. But another observation is seemingly more interesting, the LPD is always better than only NN. By investigating the other calculations we have done (plots found in  [Attachement](#attachement)), we find the same pattern, therefore we conclude that LPD brings us a real benefit for this particular application.

&nbsp;

```{r sharpe, fig.align='center', out.width='80%', fig.cap="Sharpe ratios of different signaling combinations. Based on the performance measure Sharpe ratios, the LPDs lead to a better performance. According to the Sharpe ratio, the performance of the NN+LPD signaling rule with a $\\kappa$ of 0.2, is better than Buy and Hold.",echo=FALSE, fig.width = 8, fig.height = 5}
# knitr::include_graphics("images/xailpd/lambda_1/sharpe.svg")

load("data/xai/7_7_withsignal_xai_in/sharpeplot_without_eth.rda")
defaultmar <- par("mar")
par(mar=c(11,4,4,4))
my_bar <- barplot(as.numeric(sharpesave), border=F,
                  names.arg=c(TeX(sprintf("NN $\\kappa = %.1f$", 0.1)),
                              TeX(sprintf("LPD $\\kappa = %.1f$", 0.1)),
                              TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.1)),
                              TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.1)),
                              
                              TeX(sprintf("NN $\\kappa = %.1f$", 0.2)),
                              TeX(sprintf("LPD $\\kappa = %.1f$", 0.2)),
                              TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.2)),
                              TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.2)),
                              
                              TeX(sprintf("NN $\\kappa = %.1f$", 0.3)),
                              TeX(sprintf("LPD $\\kappa = %.1f$", 0.3)),
                              TeX(sprintf("NN+LPD $\\kappa = %.1f$", 0.3)),
                              TeX(sprintf("NN+LPD+GARCH $\\kappa = %.1f$", 0.3)),
                              
                              TeX(sprintf("Buy and Hold"))),
                  cex.names=0.8,
                  las=2, 
                  col=colors, 
                  ylim=c(0,4), 
                  main=TeX(sprintf("Sharpe $\\lambda = %d$", 1)),
                  ylab="Sharpe")
grid()
par(mar=defaultmar)
```



Figure \ref{fig:batch} gives an insight on how the individual combinations performed in each of the 9 splits. The NN+LPD with $\kappa = 0.2$ is predominantly better in the first three splits, in the middle section its worse and towards the end the Sharpe is again better than buy and hold.
But we have to take this plot with a grain of salt because these splits are only one month out of sample, meaning the Sharpe Ratio is sensitive to outliers, this effect is even stronger when there are few datapoints present.

&nbsp;

```{r batch, fig.align='center', out.width='80%', fig.cap="Sharpe ratios from the same different signaling combinations as in the last two graphs, but each for the 9 different time splits. Certain combinations have very similar decisions and thus lead here also to similar trajectories of the Sharpe ratios.",echo=FALSE, fig.width = 8, fig.height = 5}
# knitr::include_graphics("images/xailpd/lambda_1/batch.svg")

load("data/xai/7_7_withsignal_xai_in/sharpmat_perbatch_without_eth.rda")
load("data/xai/7_7_withsignal_xai_in/allsharp_without_eth.rda")


colorsbatch= c("red","pink","violetred","blue","lightblue","turquoise","darkorange","goldenrod1","yellow","green")
main="Sharpe per batch, λ=1"
plot(sharpmat_1[,4],type="l",col="green",xlab="Batch Nr.",ylab= "Sharpe",lwd=2,ylim=c(min(allsharp),max(allsharp)),
     main=TeX(sprintf("Sharpe per batch, $\\lambda = %d$", 1)))
grid()
for (i in 1:8){lines(allsharp[,i],col=colorsbatch[i],type="l",lwd=2)}
```

\newpage

#### 3.3.3. Adding Ether
&nbsp;

In the performance plot \ref{fig:perf}, we have still left out what to do with 0 signals. One opportunity as in [3.3](#trading-strat) shortly mentioned, is to invest the money in another asset. By reason of staying in the same asset class we choose Ether [@ether] to invest in.
The coin Ether from Ethereum is with $61.48B [@ethereum] the second largest cryptocurrency according to market capitalisation. 

According to [@correlation_btc] the correlation between the two assets is 0.91. This is not a braking factor to us, because the 0 signal of the LPD is just indicating that something is changing in the data or the neural net is not stable, thus we can not exactly know what the 0 signal means. 

The structure Ether is very similar to Bitcoin [@oeko3project]. The series is imported in the same way as BTC e.g. the log returns are derived.

That said, we use a simple rule to integrate Ether. If the signal of the combination NN+LPD is 0 then we go long on Ether. As soon as the signal from LPD changes back to 1 or -1, Ether is sold and the Bitcoin position is entered again.

&nbsp;

```{r with_eth, fig.align='center', out.width='80%', fig.cap="As described in this section, we investigate whether the addition of an alternative investment can lead to better performance. The trading signals with different kappas resulting from the neural networks and LPD (LPD+NN) are supplemented with occasional investments in ETH. The combination LPD+NN+ETH with a kappa of 0.2, clearly performs best here. Overall, even 3 combinations are better than buy and hold.",echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}
# knitr::include_graphics("images/xailpd/lambda_1/lpd_nn_eth_perf.svg")
load("data/xai/7_7_withsignal_xai_in/performance_with_eth.rda")
load("data/xai/7_7_withsignal_xai_in/nn_lpd_without_eth.rda")
signal <- data$BTC.USD.Close
signal$BTC.USD.Close <- nn_lpd
colors= c("green","red","pink","violetred","darkorchid","blue","lightblue")
plot(data, col=colors, main=TeX(sprintf("Performance cumulated from 9 splits, $\\lambda = %d$", 1)))
addLegend("topleft", 
          legend.names=c(TeX(sprintf("Buy and Hold, $Sharpe = %.2f$", 3.57)),
                         TeX(sprintf("LPD+NN $\\kappa = %.1f$, $Sharpe = %.2f$", 0.1, 2.54)),
                         TeX(sprintf("LPD+NN+ETH if 0 $\\kappa = %.1f$, $Sharpe = %.2f$", 0.1, 3.47)),
                         TeX(sprintf("LPD+NN $\\kappa = %.1f$, $Sharpe = %.2f$", 0.2, 3.76)),
                         TeX(sprintf("LPD+NN+ETH if 0 $\\kappa = %.1f$, $Sharpe = %.2f$", 0.2, 4.41)),
                         TeX(sprintf("LPD+NN $\\kappa = %.1f$, $Sharpe = %.2f$", 0.3, 3.40)),
                         TeX(sprintf("LPD+NN+ETH if 0 $\\kappa = %.1f$, $Sharpe = %.2f$", 0.3, 3.80))),
                         
          col=colors,
          lty=c(rep(1,13),2),
          lwd=c(rep(2,13),3),
          ncol=1,
          bg="white")
```

&nbsp;

In figure \ref{fig:with_eth} the NN+LPD's from figure \ref{fig:perf} and the adding of Ether is visualized. In the lower part, we can observe the signals which are 0 in the NN+LPD model. These zeroes are now complemented with the log returns from Ether.

The one performance with NN+LPD+ETH with $\kappa = 20 \%$ seems promising. Also for the other two NN+LPD the adding of Ether seems benefital.

\newpage

```{r with_eth_sole, fig.align='center', out.width='80%', fig.cap="Visualization of the best performing signaling and the benchmark buy and hold. One can clearly see that as of August 2020, the purple signaling strongly contrasts with the green benchmark. There are some decisions which lead to a better performance than simple buy and hold.",echo=FALSE, fig.width = 8, fig.height = 5, fig.keep='last'}

load("data/xai/7_7_withsignal_xai_in/performance_with_eth.rda")
load("data/xai/7_7_withsignal_xai_in/nn_lpd_without_eth.rda")
signal <- data$BTC.USD.Close
signal$BTC.USD.Close <- nn_lpd
colors= c("green","darkorchid")
plot(data[,c(1,5)], col=colors, main=TeX(sprintf("Performance cumulated from 9 splits, $\\lambda = %d$", 1)))

addLegend("left",
          legend.names=c(TeX(sprintf("Buy and Hold, $Sharpe = %.2f$", 3.57)),
                         TeX(sprintf("LPD+NN+ETH if 0 $\\kappa = %.1f$, $Sharpe = %.2f$", 0.2, 4.41))),
          col=colors,
          lty=c(rep(1,13),2),
          lwd=c(rep(2,13),3),
          ncol=1,
          bg="white")

events<- xts(LETTERS[1:6], as.Date(c("2020-08-01","2020-09-04","2020-10-18","2021-01-15","2021-02-22","2021-03-12")))
addEventLines(events, srt=0, pos=1, lty=3, col = rep("#004c6d",6),lwd=2, cex=1.2)
# lines(signal, on=NA, lwd=2, col="red", ylim=c(-1.3, 1.3))

```

&nbsp;

For a closer analysis, let us have a look at the cumulated log-returns of the best performing model vs. buy-and-hold in Figure \ref{fig:with_eth_sole}. The following analysis is related to the violet chart and starts from left to right, beginning with the letter **A**. 
Immediately after the line by letter **A**, several decisions differ from buy-and-hold and force a shift to the series. In the letter **B**, we observe similar behavior for a short period. The unsatisfying trend change in the letter **C** is stopped around the first of November. Thereafter unto the point in letter **D** the decisions of the model behave quite similar to the green line. Right after **D** the net handles the negative trend very well and climbs further up. The same accounts for the letter **E**, where the trend once more goes in the opposite direction than the benchmark. Shortly after **F** the model does exactly the same as buy-and-hold.

Let us have a simple theoretical comparison example under the assumptions in [3.3](#trading-strat) at last.

With summarizing the logreturns from \ref{eq:logreturn} with Formula \ref{eq:logreturns_transforming}, one can calculate the value $S_{tn}$ which we end up with, when an Amount $S_{t0}$ is invested at the beginning.

&nbsp;

```{=tex}
\begin{align} \label{eq:logreturns_transforming}
S_{tn} &=S_{t0}\prod_{k=1}^{t_{n}}e^{LogReturn_{k}} \\
&=S_{t0}e^{ \sum_{k = 1}^{t_{n}} LogReturn_{k} } \nonumber
\end{align}
```

&nbsp;

Lets say, we invest 1000 US-Dollars  at the beginning of the timeperiod in \ref{fig:with_eth_sole}. With buy-and-hold we would end up at 6003.25 US-Dollars by the end of March. However if we invest the 1000 USD in the best performing strategy we will end up with **11'633.70**  USD, that would be as nearly as double the amount of buy-and-hold. Comparing the Sharperatios  **4.41** to 3.57 of the LogReturns, the used strategy does also significantly better.  



